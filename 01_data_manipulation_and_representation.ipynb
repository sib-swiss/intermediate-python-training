{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1 - importing, manipulating, and representing data \n",
    "----------------------------------------------------------------------------------------\n",
    "\n",
    "The basis of any statistical analysis is the underlying data.\n",
    "\n",
    "A data-set is typically presented as a file containing information formatted as a table:\n",
    " * each line correspond to an observation ( individual, sample, ... )\n",
    " * each column correspond to a measured variable ( height, sex, gene expression, ... )\n",
    "\n",
    "\n",
    "To read tabulated data and manipulate them, we will rely on **[pandas](https://pandas.pydata.org/)**, \n",
    "a \"high-level\" module designed for statistics/exploratory data analysis. A great strength of pandas is its **DataFrame** which emulates many of the convenient behavior and syntax of their eponym counterpart in the **R** language.\n",
    "\n",
    "To graphically represent data, we will rely on **[seaborn](https://seaborn.pydata.org/index.html)**.\n",
    "Seaborn is designed to work hand-in-hand with pandas DataDrame to produce **efficient data representation** from fairly simple commands. The seaborn official website proposes [very good tutorials](https://seaborn.pydata.org/tutorial.html) as well as a [gallery](https://seaborn.pydata.org/examples/index.html) with associated code to get you started quickly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"All modules loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "## making the plotted labels a bit bigger for presentation with a projector\n",
    "font = {'family' : 'DejaVu Sans',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 16}\n",
    "\n",
    "mpl.rc('font', **font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToC <a id='toc'></a>\n",
    "\n",
    "\n",
    "1. [Reading the data](#reading)\n",
    "\n",
    "    1.1. [the basics](#reading.1)\n",
    "\n",
    "    1.2. [header or not header, that is the question](#reading.2)\n",
    "\n",
    "    1.3. [setting up the row index](#reading.3)\n",
    "\n",
    "    1.4. [other options](#reading.4)\n",
    "\n",
    "    1.5. [more formats](#reading.5)\n",
    "\n",
    "2. [data manipulation](#manip)\n",
    "\n",
    "    2.1. [first contact with the data](#manip.1)\n",
    "\n",
    "    2.2. [accessing specific parts of the data - rows and columns](#manip.2)\n",
    "\n",
    "    2.3. [accessing specific parts of the data - selection](#manip.3)\n",
    "\n",
    "    2.4. [Operations on columns](#manip.4)\n",
    "    \n",
    "    2.4. [adding/removing and combining columns](#manip.5)\n",
    "\n",
    "3. [data description and representation](#descr)\n",
    "\n",
    "    3.1 [basic description - common summary statistics](#descr.1)\n",
    "\n",
    "    3.2 [Representing one column - histograms and density line](#descr.2)\n",
    "\n",
    "    3.x.[Interlude : multi-panel figures](#multi)\n",
    "\n",
    "    3.3 [accounting for categories in the data](#descr.3)\n",
    "\n",
    "    3.4 [representing the relationship between 2 numerical variables](#descr.4)\n",
    "\n",
    "4. [writing data and plot](#writing)\n",
    "\n",
    "5. [Free form exercise](#exo)\n",
    "\n",
    "6. [tips and tricks](#tricks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading tabulated data <a id='reading'></a> \n",
    "\n",
    "## 1.1 The basics <a id='reading.1'></a> \n",
    "\n",
    "What is the file name? location?\n",
    "What is the saprator between fields?\n",
    "\n",
    "**`pd.read_table()`** is the generalist pandas function to read tabulated data files. Aside from the name of the file to read, here are some useful optional arguments:\n",
    "* `sep`: separator between columns (by default `\"\\t\"`)\n",
    "* `header`: row number(s) to use as the column names. By default the first line is used as header.\n",
    "  Use `header=None` if the file does not contain column names.\n",
    "* `skiprows`: Line numbers to skip (0-indexed) or number of lines to skip (int) at the start of the file.\n",
    "* (true_values/false_values ??)\n",
    "\n",
    "Of course you can learn (much) more using `help(pd.read_table)`.\n",
    "\n",
    "\n",
    "\n",
    "Let's try to load the `data/titanic.csv` file. As its name suggest, this table contains data about the ill-fated [Titanic](https://en.wikipedia.org/wiki/Titanic) passengers, travelling from England to New York in April 1912. The data file is named `\"titanic.csv\"` and like its extension suggests, it contains **C**omma-**S**eparated **V**alues.\n",
    "\n",
    "Note the usage of the `head()` method of a DataFrame to display only its first 5 lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_table(\"data/titanic.csv\") \n",
    "df.head()                                 # By default, head() returns the 5 first lines of a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not look so great...\n",
    "\n",
    "### Micro-Exercise\n",
    "* Try to fix the cell just above by playing with the option(s) of `pd.read_table`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>do not scroll or be spoiled!</center>\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have just seen, `pd.read_table()` expects the input data to be **tab-delimited** (by default). Since this is not the case of the `titanic.csv` file, each line was treated as a single field (column), thus creating a DataFrame with a single column.\n",
    "\n",
    "As implied by its `.csv` extension (for \"comma-separeted values\"), the `titanic.csv` file contains **comma-delimited** values.  \n",
    "To load a CSV file, we can either:\n",
    "* Specify the separator value in via the **`sep`** argument: `pd.read_table(<file name>, sep=\",\")`.\n",
    "* Use **`pd.read_csv()`**, a function that uses commas as default separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(\"data/titanic.csv\", sep=\",\") \n",
    "# Alternatively: df = pd.read_csv(\"data/titanic.csv\", sep=\",\") \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "[back to toc](#toc)\n",
    "\n",
    "## 1.2 Header or not header, that is the question <a id='reading.2'></a>\n",
    "\n",
    "\n",
    "Another important aspect of reading data is whether your dataset has a header or not. \n",
    "By default, **`pd.read_table()` expects the first line to be a header**, unless you either:\n",
    " * Use the argument **`header=None`**.\n",
    " * Specify column names using the **`names`** argument.\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's try to load a dataset that does not have any header:  \n",
    "> *Note:* as illustrated in the cell below, `head()` can print any number of lines by passing a\n",
    "  value to it - here the first 3 lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(\"data/titanic_no_header.csv\", sep=\",\") \n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the **values in the first line were used as column names**... this is not ideal.\n",
    "\n",
    "Let's correct this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(\"data/titanic_no_header.csv\", \n",
    "                   sep=\",\", header=None) \n",
    "df.head(n=3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! \n",
    "\n",
    "Let's go one step further and **assign our own column names** using the `names` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(\"data/titanic_no_header.csv\", sep=\",\", \n",
    "                   names = [\"name\", \"column2\", \n",
    "                            \"age\", \"column4\", \n",
    "                            \"blip\", \"bloop\", \"spam\", \"eggs\"]) \n",
    "# As you can see, we can choose our own names, whether they make sense or not.\n",
    "\n",
    "df.head(3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to toc](#toc)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## 1.3 Setting the row index <a id='reading.3'></a>\n",
    "\n",
    "Now that we learned to change column names, let's see how to modify **row names**, which are called the **index**.\n",
    "\n",
    "> *Note:* not all dataset need a custom index. Oftentimes the default index (numbers starting from 0 \n",
    "  that correspond to line positions) is enough. It can often also be advantageous to have the index\n",
    "  correspond to row positions.\n",
    "\n",
    "There are several options available to modify the index that we will illustrate below.\n",
    "\n",
    "### 1.3.1 Input file contains row names (file has one less column names than data fields)\n",
    "\n",
    "Depending on how the data file was produced, it does sometimes already contain row names.\n",
    "\n",
    "Let's see an example of this with the `data/titanic_implicit_index.csv` file. First we will display the \"raw\" content of our file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/titanic_implicit_index.csv\", \"r\") as f:\n",
    "    for x in range(5):\n",
    "        print(f.readline().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, the 1st line of the file contains 8 elements, while all other lines contain 9 elements! This is because the first field of the file contains the row names.\n",
    "\n",
    "> Note: if you are using a Linux or MacOS machine, you could also use the one-liner  \n",
    ">  `!head -n 4 data/titanic_implicit_index.csv`  \n",
    "> (the leading `!` is a special Jupyter Notebook syntax that indicates a call to a shell program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(\"data/titanic_implicit_index.csv\", sep=\",\") \n",
    "df.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index[:5]  # access the index directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When pandas detects this configuration (one more field than column names), it automatically uses the first, nameless, column as index.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 1.3.2 Using `index_col` to manually specify the index\n",
    "\n",
    "Pandas can be instructed to use a specific column of an input file as index via the **`index_col`** argument.\n",
    "\n",
    "As shown below, the index column can be indicated either:\n",
    "* By position (*reminder:* column indexing is zero-based).\n",
    "* By name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Index column is specified by position: 0 = 1st column.\n",
    "df = pd.read_table(\"data/titanic.csv\", sep=\",\", index_col=0)\n",
    "df.head(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index column is specified by name.\n",
    "df = pd.read_table(\"data/titanic.csv\", sep=\",\", index_col=\"Name\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Note:* removing the name of the index (\"Name\" in the example above) can be done\n",
    "  with: `df.index.name = None`\n",
    "  \n",
    "> *Note:* pandas also has a system of multiple, hierarchised indexing. This is, however, a much\n",
    "  more specialized and advanced feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to toc](#toc)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## 1.4 Other `pd.read_table()` options <a id='reading.4'></a>\n",
    "\n",
    "`pd.read_table()` has a vast arrays of options. We cannot go though all of them, but here are a few which may be of interest to you:\n",
    "* `true_values`/`false_values`, each a list. A must if you have columns encoded with \"yes\"/\"no\" labels.\n",
    "* `na_values`: takes a list. Ideal when your NAs are encoded as something unusual (eg, `.`,` `,`-9999`,...).\n",
    "* `parse_dates`/`infer_datetime_format`/`date_parser`: options to help you handle date parsing, which can\n",
    "  otherwise be a nightmare - [more on this here](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html).\n",
    "* `compression`: your data is in a compressed format (zip, gzip, ...)? Not a problem!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to toc](#toc)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## 1.5 Reading other tabulated file formats <a id='reading.5'></a>\n",
    "\n",
    "As you might expect, pandas is not limited to text, csv/tsv-like files.\n",
    "\n",
    "* `pd.read_excel()`\n",
    "* `pd.read_json()`\n",
    "* `pd.read_sql()` \n",
    "* ... see [here for an exhaustive list of pandas reader and writer functions](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genbank_df = pd.read_json(\"data/genbank.sub.ndjson\", lines=True)\n",
    "genbank_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Micro-Exercise:\n",
    "* Read the file `data/pbmc_data.countMatrix.50.txt.zip` as a DataFrame. Determine which is the separator,\n",
    "  and decide whether there is a header and/or an index column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to toc](#toc)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 2. Data manipulation <a id='manip'></a>\n",
    "---------------------------------\n",
    "\n",
    "Now that you know (almost) everything there is to know about loading your data files as a `DataFrame`, let's see what we can actually do with these!\n",
    "\n",
    "\n",
    "## 2.1 First contact with the data <a id='manip.1'></a>\n",
    "\n",
    "Gathering basic information about a DataFrame is fairly easy, and we will illustrate this with the titanic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(\"data/titanic.csv\", sep=\",\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* **`df.shape`** returns a tuple with the numbers of rows and columns: `(row_count, col_count)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberRows, numberCols = df.shape\n",
    "print('row count:', numberRows, '\\ncolumn count:', numberCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* Column names can be accessed with the **`df.columns`** attribute, and the index with the\n",
    "  **`df.index`** attribute:\n",
    "\n",
    "> *Note:* here the index is number-based, starting with 0 (this is the default pandas indexing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('column names:\\n', df.columns)\n",
    "print('index:\\n', df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* The **`df.columns`** and **`df.index`** attributes can also be used to set new values for column names\n",
    "  and index labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [x.upper() for x in df.columns]\n",
    "df.index = [\"passenger_\" + str(i) for i in df.index]\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's reset out changes in index and column names:\n",
    "df.columns = df.columns.str.capitalize()\n",
    "df.index = range(0, df.shape[0])\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Note:* how did we apply a `str` function to all the column names at once?\n",
    "  That is a very powerful feature, which we'll discuss later.\n",
    "\n",
    "<br>\n",
    "\n",
    "Each column of a DataFrame has an associated **data type**, which controls the operations you may perform on it. Our example DataFrame contains the following data types:\n",
    "* `object` : catch all for text, intermixed or not with numbers\n",
    "* `float64` : float\n",
    "* `int64` : integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"columns types:\\n\", df.dtypes, sep=\"\")  # list the type of each column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other types exist. The main types are :\n",
    "    \n",
    " * object : catch-all type\n",
    " * int64 : integers\n",
    " * float64 : floats\n",
    " * bool : booleans\n",
    " * datetime64 : date and time points \n",
    " * category : categorical data with fixed possible values\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To change the type of a column, the simplest is to use the `.astype` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## float to object \n",
    "FareStr = df.Fare.astype(str)\n",
    "## it can then be manipulated as a string:\n",
    "FareStr = '$' + FareStr\n",
    "FareStr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Micro-Exercise:\n",
    "\n",
    "A typical case: the data you were given is badly formatted and you have to cure this a bit. For example, a column contains percentages with the % sign...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I just generate the data manually\n",
    "PercentColumn = pd.Series( np.random.randint(0,100,100), dtype=\"str\") +'%' \n",
    "PercentColumn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your mission: get this column into something usable as numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "In the case of the titanic data, the passenger class is not really a number, and may be better represented as a `category` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Pclass = df.Pclass.astype(\"category\")\n",
    "df.Pclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then category-specific functions can be accessed with `.cat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for instance, I may want to change the labels a bit here:\n",
    "df.Pclass= df.Pclass.cat.rename_categories( {1:'I',\n",
    "                                             2:\"II\",\n",
    "                                             3:\"III\"})\n",
    "df.Pclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[datetime64](https://pandas.pydata.org/docs/user_guide/timeseries.html) and [category](https://towardsdatascience.com/staying-sane-while-adopting-pandas-categorical-datatypes-78dbd19dcd8a) are fairly specific, and we refer you to the provided links if you want to learn more about them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## 2.2 DataFrame subsetting - accessing specific rows and columns <a id='manip.2'></a>\n",
    "\n",
    "### 2.2.1 Accessing a single column\n",
    "\n",
    "One can access a column just by using the syntax **`df[<column name>]`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sex']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Note:* `pandas` only prints first and last 5 rows of the column to avoid clogging your screen, as well as some useful info.\n",
    "\n",
    "Alternatively, we can also use the syntax **`df.<column name>`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Sex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2.2.2 Subsetting a DataFrame with the `loc[]` and `iloc[]` indexers\n",
    "\n",
    "A very common operation to perform on DataFrames is to create a subset by selecting certain rows and/or columns.  \n",
    "There are 2 methods in pandas to perform a selection on a DataFrame (here `df`):\n",
    "* **position based:** using `df.iloc[<row selection>, <column selection>]`\n",
    "* **index/label based:**  using `df.loc[<row selection>, <column selection>]`\n",
    "\n",
    "![image.png](img/pandas_position_vs_index_selection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select all rows/or columns, the symbol `:` can be used as row or column selection. It works with both `.loc[]` and `.iloc[]`:\n",
    "* `df.loc[<row selection>, :]` - select all columns.\n",
    "* `df.loc[:, <column selection>]` - select all rows.\n",
    "\n",
    "> *Note:* when selecting on rows only (i.e. select all columns), the `df.loc[<row selection>, ]`\n",
    "  and `df.loc[<row selection>]` syntaxes are also possible. The `:` is not compulsory in that case).\n",
    "\n",
    "<br>\n",
    "\n",
    "**Common pitfall with slicing:** `loc[]` includes the end index, but `iloc[]` does not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[ 0:3 , : ]    # this selects the first 4 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[ 0:3 , : ]   # this selects the first 3 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Examples:** let's apply all we have learned about `loc[]` and `iloc[]`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first 3 rows, and all columns.\n",
    "df.loc[0:3, : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first 3 rows, and columns 'town name' and 'Total'.\n",
    "df.loc[ : , ['Sex' , 'Name' , 'Name' , 'Age'] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Note:* I am free to select a column several time, in whichever order I wish. This can thus be used to\n",
    "  re-order columns in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[ :, \"Name\":\"Fare\" ]   # select all rows, and columns from column 'Name' to column \"Fare\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0:2, [0,2,3]]         # select the first 2 rows, and columns 0,2 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0, [0,2,3]]           # select the first row, and columns 0,2 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Question:** what type of object do we get when selecting a single row/column? You may have noticed that it does not get represented as a list, so what is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a single row:\n",
    "row_4 = df.iloc[3,]\n",
    "row_4b = df.loc[3,]\n",
    "print(type(row_4))\n",
    "print(type(row_4b))\n",
    "\n",
    "# Select a single column. Note that when selecting by columns only, \":\" must be used to indicate\n",
    "# that all rows should be selected.\n",
    "col_age_a = df.loc[:,\"Age\"]\n",
    "col_age_b = df.iloc[:,3]\n",
    "col_age_c = df[\"Age\"]          # When selecting based on columns only, using this syntax is simpler.\n",
    "print(type(col_age_a))\n",
    "print(type(col_age_b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are pandas **`Series`**. They are the equivalent of `DataFrame`, but **1-dimensional**.\n",
    "Their elements can be accessed in quite a similar way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( row_4 )\n",
    "print( \"\\n---\\n\" )\n",
    "print( row_4[0] )   # by position\n",
    "print( \"\\n---\\n\" )\n",
    "print( row_4.Age )  # by name "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Micro-Exercise\n",
    "* Select all odd rows from the Titanic data frame, as well as the columns \"Name\", \"Age\" and \"Fare\".\n",
    "* Re-order the columns so that \"Age\" is first and \"Name\" is second.\n",
    "\n",
    "**Hint:** we will see how to perform conditonal selection later, but for now you can use the `range()` function to help you with this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "[back to toc](#toc)\n",
    "\n",
    "## 2.3 Accessing specific parts of the data - selection <a id='manip.3'></a>\n",
    "\n",
    "Another, powerful, way of accessing specific part of the data is by defining a mask, which will **filter the data through a particular condition**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maskMale = df['Sex'] == 'male'  \n",
    "maskMale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mask (a `Series` object), is in effect a list of values that are `True` or `False` depending on whether or not they satisfy the defined condition (here, `Sex` is equal to \"male\").\n",
    "\n",
    "<br>\n",
    "\n",
    "A great method of `Series` containing categorical kind of data (such as `True`/`False` values only) is **`values_counts()`**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maskMale.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mask can also be applied to the `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[maskMale, ['Sex','Fare','Survived']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that only the 577 rows corresponding to male passengers have been selected.\n",
    "\n",
    "<br>\n",
    "\n",
    "Masks may be combined to produce more complex selection criteria, using **`&`** (logical and) and **`|`** (logical or)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# male passenger with a fare > 200\n",
    "mask = (df[ 'Sex' ] == 'male') & ( df.Fare > 200  ) \n",
    "df.loc[ mask , ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all people that are either < 25 or a women.\n",
    "df.loc[(df.Age < 25) | (df.Sex == \"female\"), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Reminder: `.iloc[]` does not support boolean results (True/False) for row selection**: it requires row positions. One can hack its way through that by calling the index of a mask, or using the [query method](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Micro-Exercise:\n",
    "* Select the fare and name of passengers in first class (`Pclass` \"I\" ) which are less than 18 years old."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "[back to toc](#toc)\n",
    "\n",
    "## 2.4 Operations on columns <a id='manip.4'></a>\n",
    "\n",
    "Pandas DataFrame allow to use arithmetic operators on columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Age\"] = df[\"Age\"] + 1\n",
    "df.Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Age\"] -= 1   # same as df[\"Age\"] = df[\"Age\"] - 1\n",
    "df.Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This syntax becomes quite powerful as we can now very easily apply opperations accross whole columns.\n",
    "\n",
    "For instance, consider this data from the 1880 swiss census:\n",
    "* The `Total` column gives the total number of registered inhabitants in the given town\n",
    "* The `Male` columns gives the number of men."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_census = pd.read_table(\"data/swiss_census_1880.csv\", \n",
    "                          sep=\",\")\n",
    "df_census.loc[:5, [\"town name\", \"Total\", \"Male\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "To get the fraction of men in each town, we can now simply write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_census.Male / df_census.Total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Better yet: we can assign our result to a **new column as if we were adding a key to a dictionnary**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_census['Male Fraction'] = df_census.Male / df_census.Total\n",
    "df_census.loc[:, [\"town name\", \"Total\", \"Male\", \"Male Fraction\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "These row value assignment operations may be combined with a selection operation, allowing e.g. to modify the values of certain rows in a DataFrame based on a certain condition.\n",
    "\n",
    "This is particularly useful when you want to mark some data as NAs for instance. Let's magine that for some reason the fares of class 3 are not valid. We want to set them to NA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NA is represented using pd.NA\n",
    "df.loc[ df.Pclass==\"III\" , 'Fare'] = pd.NA\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Micro-Exercise:\n",
    "* Children under the age of 10 get a special discount of 50% on their fare.\n",
    "  Apply this by dividing by 2 the `Fare` of eligible passenger in the `df` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "As we have just seen, pandas DataFrame allow easy arithmetics on the elements of a column.\n",
    "\n",
    "But it is also possible to apply any custom function on the elements of a column using the `.map()` method. E.g.: `df[\"Survived\"].map(func)`\n",
    "\n",
    "<br>\n",
    "\n",
    "**Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sillyFunction( x ):\n",
    "    ''' returns \"even\" if x is divisible by 2, otherwise returns \"odd\" '''\n",
    "    if x % 2 == 0 :\n",
    "        return 'even'\n",
    "    return 'odd'\n",
    "\n",
    "print( 2 , '->' , sillyFunction(2) )\n",
    "print( 3 , '->' , sillyFunction(3) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for each value in the age column, apply the sillyFunction\n",
    "## NB : THIS DOES NOT MODIFY THE CONTENT OF THE DATAFRAME\n",
    "df.Age.map( sillyFunction )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### interlude: copy or not copy?\n",
    "\n",
    "What happens if we select part of a DataFrame and modify it? does the original data stays the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(\"data/titanic.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.Sex == \"male\", : ].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.Sex == \"male\", \"Age\" ] = 999 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well OK, the age was changed from the slice to the main `DataFrame` : `pandas` avoid doing copies when it can... BUT :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_maleOnly = df.loc[ df.Sex == 'male' , : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the age to 888 in the dataframe of male only\n",
    "df_maleOnly.Age = 888"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's this? We get a warning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_maleOnly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed the change made to `df_maleOlny` has not been reflected to `df`.\n",
    "\n",
    "\n",
    "Sadly, it is not always that easy to get when you get a **view** or a **copy**.\n",
    "\n",
    "![image.png](img/view_copy.png)\n",
    "\n",
    "\n",
    " * **view:** this still point to the original data.\n",
    " * **copy:** new data. Modifying this leaves the original data untouched.\n",
    "\n",
    "In general, **using `.loc[]` should return a view**, however that also depends on the evaluation order of some of the performed operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does this line create a copy or a view? Nobody knows.\n",
    "df_maleOnly = df.loc[ df.Sex == \"male\", : ] \n",
    "\n",
    "\n",
    "# Only when we try to set some data does pandas detect something potentially fishy and warns us...\n",
    "df_maleOnly.Age=888"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a copy, we must make it explicit to pandas by using the **`copy()`** method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a copy of the DataFrame returned by .loc[]\n",
    "df_maleOnly = df.loc[ df.Sex == 'male' , : ].copy()\n",
    "\n",
    "\n",
    "# Now we don't get a warning anymore:\n",
    "df_maleOnly.Age=888"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This issue is quite complex, but as you are likley to encounter this warning at some point, it is better to have the cat out of the bag now. Here is a recommended reading if you wish to get a more [in-depth explaination of view vs. copy](https://www.dataquest.io/blog/settingwithcopywarning).\n",
    "\n",
    "\n",
    "[back to toc](#toc)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## 2.5 Adding/removing and combining columns <a id='manip.5'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table( \"data/titanic.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already seen how to create a new column: associate a column name (key) and a container (value) with a number of elements corresponding to the number of rows in the `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random as rd\n",
    "\n",
    "df['discount'] = rd.random(df.shape[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create a column from existing ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Discounted_Fare'] = ( 1 - df.discount ) * df.Fare\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing columns is about as easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='discount' , inplace=True)  # use the 'index' argument to remove rows instead\n",
    "print(\"is 'discount' part of the columns : \" , 'discount' in df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how we gather the value returned by `df.drop` to get the data frame without the column.\n",
    "This can be circumvented with `inplace=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'Discounted_Fare'\n",
    "df2 = df.drop(columns=col) \n",
    "\n",
    "print(\"is '\",col,\"' in df : \" , col in df.columns , sep='')\n",
    "print(\"is '\",col,\"' in df2 : \" , col in df2.columns , sep='')\n",
    "## default behavior : df is unchanged, df2 is the oone without the column\n",
    "\n",
    "print('---')\n",
    "df2 = df.drop(columns=col, inplace = True) \n",
    "\n",
    "print(\"is '\",col,\"' in df : \" , col in df.columns , sep='')\n",
    "print(\"df2 is \",df2)\n",
    "## inplace = True : df is changed, df2 is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help( df.drop )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "If you want to **add rows**, then you may use **`pd.concat()`**, which takes a list of `DataFrame` or `Serie` :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat( [df , pd.DataFrame({'Name':'The Mr. Doctor',\n",
    "                 'Sex':'fluid',\n",
    "                 'Age':pd.NA,\n",
    "                 'Pclass':2,\n",
    "                 'Survived':pd.NA,\n",
    "                 'Family':pd.NA,\n",
    "                 'Fare':42.00,\n",
    "                 'Embarked':pd.NA}, index=[df.shape[0]]) ] )\n",
    "df2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.drop(index=891, inplace=True)\n",
    "df2.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Exercise 1 - data manipulation\n",
    "\n",
    "Using the Titanic dataset:\n",
    "\n",
    "1. Select passengers which survived. How many are males/females?\n",
    "2. Create a new column `Title` is the `DataFrame` representing the title by which passengers should\n",
    "   be addressed. The title can be found in the passenger name and is the only word ending with a `'.'`\n",
    "   \n",
    "   **hint for question 2:** there is no *easy, one-line* answer. Create a function to get the title from the name and work your way from there.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_survived = df.Survived == 1\n",
    "\n",
    "df.loc[m_survived , 'Sex'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the following to load solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 1-3 solutions/solution_01_01.py\n",
    "# 1. Select passengers which survived. How many are males/females?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 4- solutions/solution_01_01.py\n",
    "#2. Create a new column Title is the DataFrame representing the title by which passengers should be addressed. The title can be found in the passenger name and is the only word ending with a '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to toc](#toc)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 3. Data description and representation <a id='descr'></a>\n",
    "-----------------------------------------------------------\n",
    "\n",
    "## 3.1 Basic description - common summary statistics <a id='descr.1'></a>\n",
    "\n",
    "When doing exploratory analysis of a dataset, it is often useful get some basic statistics on a per-column basis (since rows will typically represent the samples and columns the explanatory variables).\n",
    "\n",
    "Pandas has a number of methods that can be applied both on an entire DataFrame or on individual columns (Series):\n",
    "* `describe()`: print summary statistics for all **numeric columns** (count, mean, min, max, std, quantiles).\n",
    "* `count()`: returns the count of values (in each column, if applied to a DataFrame), that are not NaN\n",
    "  (i.e. missing values - Not a Number).\n",
    "* `value_counts()`: returns the count of each value. Useful for discrete variables (e.g. factors).\n",
    "* `min()`, `max()`, `mean()`, `std()`, `quantile()`, `sum()`: returns the min, max, mean, etc.\n",
    "\n",
    "When applied to a DataFrame, these method compute values for all columns, skipping columns for which they cannot be applied, e.g. standard deviation can only be computed for numeric columns).\n",
    "\n",
    "> Note: by default, all these statistics are computed on rows (e.g. max value of all rows in a given column), but they can also be computed by row by adding the `axis=1` argument. E.g. `df.max(axis=1)` returns the max value among columns for each row.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Example:** count the number of people that embarked at different ports.\n",
    "> *Note:* embarked - Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Embarked.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another nice trick to know about, you can create contigency tables to look at the relationship between categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df.Pclass , df.Survived)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Example:** compute the mean, median, min and max fare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean Fare  ',df.Fare.mean())\n",
    "print('median Fare',df.Fare.median())\n",
    "print('min Fare   ',df.Fare.min())\n",
    "print('max Fare   ',df.Fare.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Of course we can combine this with selections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_male = df.Sex == 'male'\n",
    "\n",
    "print('median fare of male', df.Fare[mask_male].median() )\n",
    "print('median fare of female', df.Fare[~mask_male].median() )\n",
    "# note the use of ~ to reverse the mask!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Another neat trick is that these methods can be used on whole DataFrames. By default they apply on each columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mean()   # use df.mean(axis=1) to compute a mean per row, here non-sensical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a \"FutureWarning\", letting us know that in the future this will change. So if we correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Age\",\"Pclass\",\"Family\",\"Fare\"]].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Micro-Exercise:\n",
    "* Compute the mean fare for each passenger class (`Pclass`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Micro-Exercise:\n",
    "* Using the dataset of single cell data `data/pbmc_data.countMatrix.50.txt.zip`:\n",
    "  1. Compute the sum for each column.\n",
    "  2. Normalize each column by dividing its values by the column-wise sum.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sc = pd.read_table('data/pbmc_data.countMatrix.50.txt.zip', sep=\" \", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "A very useful method of DataFrame to get an **overview of a dataset** is **`df.describe()`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `df.describe()` function gives information about all numerical columns in the dataset at once (note that non-numerical values are absent here).\n",
    "\n",
    "It is very useful not only to get a first impression on the dataset, but also to catch eventual errors in the data: a negative number where there should be only positive values, missing values (NAs), ...\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "**BUT** the most eagle-eyed among you will have spotted a problem here... what is it? how to solve it ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_describe.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "Anyway, back to the matter at hand.\n",
    "\n",
    "**`.describe()`** gives access to some of the most commonly used summary statistics:\n",
    "* (arithmetic) **mean** : ${\\displaystyle \\bar{x}={\\frac {1}{n}}\\sum _{i=1}^{n}x_{i}}$  or, for coders : `sum(x)/len(x)`\n",
    "* **standard deviation** (a.k.a. std, stdev) : this corresponds to the average of the absolute difference to the mean. It is the **square root of the variance**.\n",
    "* **minimum** and **maximum**: smallest and biggest value among the data. Looking at them can help detect outliers.\n",
    "* **quartiles** : they correspond to the value such that\n",
    "    * 25% (first quartile, Q1), \n",
    "    * 50% (second quartile, Q2, median), or\n",
    "    * 75% (second quartile, Q3)\n",
    "    \n",
    "of the values are lower than them. They are less sensitive than the mean to outlier values.\n",
    "\n",
    "[back to toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "It is all nice to be able to see these numbers, but often data is best explored visually, which is why we will be talking about ...\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## 3.2 Representing one column - histograms and density line <a id='descr.2'></a>\n",
    "\n",
    "There exists a number of plotting libraries in python. We will mostly be using **matplotlib** and **seaborn**.\n",
    "\n",
    "**matplotlib** can be seen as a base library, which defines a lot of low-level plotting functions. **seaborn** is built on top of matplotlib, and provides more high-level functions, which interface very well with DataFrames.\n",
    "\n",
    "We will not delve in all the details of these two libraries, but will instead try to convey elements about their general philosophies and most common functions/arguments.\n",
    "\n",
    "That should give you a head start to get the most out of their very well done tutorial and galleries:\n",
    " * [matplotlib](https://matplotlib.org/) - [tutorial](https://matplotlib.org/stable/tutorials/introductory/pyplot.html#sphx-glr-tutorials-introductory-pyplot-py) - [gallery](https://matplotlib.org/stable/gallery/index.html)\n",
    " * [seaborn](https://seaborn.pydata.org/) - [tutorial](https://seaborn.pydata.org/tutorial.html) - [gallery](https://seaborn.pydata.org/examples/index.html)\n",
    "\n",
    "\n",
    "> `pandas` also proposes plotting functions, which are basically also using `matplotlib`. \n",
    "\n",
    "\n",
    "The simplest way to represent the distribution of a set of values using **seaborn** is to use its **`sns.displot()`** function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# \"displot\" is shorthand for distribution plot\n",
    "sns.displot( df.Fare )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, you should now see a histogram (in blue):\n",
    "* The *x-axis* corresponds to the fare paid by each passenger.\n",
    "* The *y-axis* corresponds to the **count**: the number of values falling in a given *bins*\n",
    "  (i.e. a bar of the histogram).\n",
    "\n",
    "By using the `kind` argument, one can change the type of plot from histogram `\"hist\"`, to density line `\"kde\"`, or cumulative distribution function `\"ecdf\"`.\n",
    "\n",
    "For instance :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot( df.Fare, kind='kde' )\n",
    "# Note: kde stands for 'kernel density estimation', which is the method used to compute this density line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that the y-axis is now in **density** units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, many options can be set in order to combine plot types and pimp your plot :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.displot(df.Age, kde=True, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Note:* having both the histogram and the density line switches the y-axis from density to count,\n",
    "  which is sometimes more interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`sns.displot()` is what we could call a **figure-level** function:\n",
    " * it has several *kind* of representation.\n",
    " * it does not play nice in **multi-panel** figures.\n",
    "\n",
    "\n",
    "[back to toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "### Interlude: multi-panel figures <a id='multi'></a>\n",
    "\n",
    "We create Multi-panel figures using `matplotlib` command and concepts. \n",
    "To make it simple, we call `fig, axes = plt.subplots( nb_rows , nb_cols )`, which creates:\n",
    " * `fig`, a `figure` object which controls the whole multipanel figure\n",
    " * `axes` : a list containing individual `axe` objects though which we can plot on individual panels of the figure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a list of 100 evenly spaced points between 0 and 10.\n",
    "x = np.linspace(0, 10, 100) \n",
    "\n",
    "# Create a figure with multiple panels: 1 row, 2 columns\n",
    "fig, axes = plt.subplots( 1, 2 , figsize=(10,8))\n",
    "\n",
    "print( axes ) # axes is a list of two axes objects\n",
    "\n",
    "## plotting on the first axe == left panel\n",
    "axes[0].plot(x, np.sin(x), label='sin')\n",
    "axes[0].set_title(\"sine plot\")\n",
    "axes[0].set_xlabel(\"value\")\n",
    "axes[0].set_ylabel(\"sine/cosine value\")\n",
    "axes[0].legend()\n",
    "\n",
    "## plotting on the second axe == right panel\n",
    "axes[1].plot(x, np.cos(x), label='cos')\n",
    "axes[1].set_title(\"cosine plot\")\n",
    "axes[1].set_xlabel(\"value\")\n",
    "axes[1].set_ylabel(\"sine/cosine value\")\n",
    "\n",
    "plt.tight_layout() ## this creates a more harmonious organization of the different panels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### back to data representation ...\n",
    "\n",
    "The different `kind` (histogram, kde, ecdf) of `sns.displot()` can all be called individually using their own functions : `histplot()`, `kdeplot()`, `ecdfplot()`. \n",
    "\n",
    "In contrast to `sns.displot()` which is **figure-level**, these more-specialized functions are *axe-level* : they play nicely in a multiple panel context.\n",
    "\n",
    "They interface with `matplotlib`'s axes using their `ax` argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating multiple panels : 1 row, 2 columns\n",
    "fig, axes = plt.subplots( 1, 2 )\n",
    "\n",
    "# Plotting on the first axe == left panel\n",
    "sns.histplot(df.Age , ax = axes[0])\n",
    "\n",
    "# Plotting on the second axe == right panel\n",
    "sns.kdeplot(df.Fare , ax = axes[1])\n",
    "\n",
    "plt.tight_layout()   # this creates a more harmonious organization of the different panels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "In **histogram**, a different number of *bins* shows different aspects of the distribution of your data and so it is important to choose one that is appropriate to your sample size and data. \n",
    "\n",
    "By default, seaborn automatically infers the number of *bins*. \n",
    "You may stick with the original or change it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's play with a different dataset for this \n",
    "dfFractions = pd.read_table(\"data/census1880_fractions.csv\", sep=\",\")\n",
    "\n",
    "# Set up a figure with multiple panels, here a 2 by 2 grid\n",
    "f, axes = plt.subplots( 2, 2, figsize=(14, 7) )\n",
    "\n",
    "# axes is now a list of list of axes, to represent the 2d nature of our panels:\n",
    "#  [ [<axe>,<axe>] ,\n",
    "#    [<ax>,<axe>]]\n",
    "\n",
    "\n",
    "# then you can specify where each plot goes on the figure with the ax argument of the ([0,0] is the top left corner)\n",
    "\n",
    "# Plot a simple histogram with binsize determined automatically\n",
    "sns.histplot(dfFractions['0-14 y.o.'], kde=False, color=\"b\",\n",
    "             ax=axes[0, 0]).set_title('automatic')\n",
    "\n",
    "# Plot a simple histogram with binsize 5, 10 , 1000\n",
    "sns.histplot(dfFractions['0-14 y.o.'], bins=5   , kde=False, color=\"b\", ax=axes[0, 1]).set_title('5 bins')\n",
    "sns.histplot(dfFractions['0-14 y.o.'], bins=10  , kde=False, color=\"b\", ax=axes[1, 0]).set_title('10 bins')\n",
    "sns.histplot(dfFractions['0-14 y.o.'], bins=1000 , kde=False, color=\"b\", ax=axes[1, 1]).set_title('1000 bins')\n",
    "\n",
    "plt.tight_layout()# this makes the panels margins and setup more graceful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So, to recap, we use `sns.histplot` instead of `sns.displot` : \n",
    " * `histplot` : gives a lower-level kind of plot which is easier to manipulate in multiple figure.\n",
    " * `displot` : high-level function, with a lot of capabilities, but does not play nice if not the top figure.\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "We can combine graphical elements coming from `matplotlib` and `seaborn` quite seamlessly.\n",
    "\n",
    "Here is a more advanced example where I represent the the **mean**, the **median** and the **mode** of a distribution on top of an histogram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Here we just define a small function for plotting a distribution with the mean median and mode \n",
    "def plotWithMeanMedianMode( dat , ax):\n",
    "    \"\"\" \n",
    "        Takes:\n",
    "            * dat : a pandas series\n",
    "            * ax : a matplotlib axe    \n",
    "    \"\"\"\n",
    "    mode=dat.mode()[0] #we only select the first mode\n",
    "    mean=dat.mean()\n",
    "    median=dat.median()\n",
    "\n",
    "    sns.histplot( dat , kde=True , ax=ax) # line for histogram and density line\n",
    "\n",
    "    ax.axvline(mean, color='r', linestyle='--' , label='Mean')\n",
    "    ax.axvline(median, color='g', linestyle='-' , label='Median')\n",
    "    ax.axvline(mode, color='b', linestyle='-' , label='Mode')\n",
    "    ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots( 2, 1, figsize=(15, 10) )\n",
    "\n",
    "plotWithMeanMedianMode( df.Fare , ax=axes[0])\n",
    "plotWithMeanMedianMode( df.Age , ax=axes[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of note here :\n",
    " * `axvline`: create a vertical line.\n",
    " * `label`: legend label associated with the element created by this function.\n",
    "   Common to a lot of matplotlib plotting functions.\n",
    " * `ax.legend()`: makes the legend appear.\n",
    " \n",
    "> You can specify where the legend goes with `loc=` one of `{'best', 'upper left', 'upper right', 'lower left', 'lower right', 'upper center', 'lower center', 'center left', 'center right', 'center'}` or a `(x,y)` tuple  (more options to manually specify elements in the `help()`)\n",
    "\n",
    "[back to toc](#toc)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## Exercise 2 - histograms\n",
    "\n",
    "Using the Titanic dataset:\n",
    "\n",
    "1. Plot the `Age` distribution among first class passengers. Try to choose an appropriate mode of \n",
    "   representation (histogram? density line? number of bins?).\n",
    "2. Make a figure with 3 panels. In the panels, plot the histogram of the `Fare` among passengers in\n",
    "   the first, second, and third class, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 1-6 solutions/solution_01_02.py\n",
    "# 1. Plot the Age distribution among first class passengers. Try to choose an appropriate mode of representation (histogram? density line? number of bins?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 7- solutions/solution_01_02.py\n",
    "#2. Make a figure with 3 panels. In the panels, plot the histogram of the `Fare` among passengers in\n",
    "#   the first, second, and third class, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "[back to toc](#toc)\n",
    "\n",
    "## 3.3 Accounting for categories in the data <a id='descr.3'></a>\n",
    "\n",
    "Here the categories would be the passenger class of sex, but depending on data-sets they could be a genotype (Wild-Type versus KO), sex, experimental condition, ...\n",
    "\n",
    "Of course, we could manually compute the means and plot an histogram for each category in our data-set, but pandas and seaborn offer much more efficient routines for this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing a summary statistics on a pandas DataFrame is done **using the `df.groupby(...)` method and the applying some function** to the result of that grouping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby('Sex')     # grouping by Sex.\n",
    "grouped['Age'].median()         # median age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we get the median age per Sex. \n",
    "\n",
    "<br>\n",
    "\n",
    "Of course here there are only two categories, but this gets very interesting when there is a lot of categories.\n",
    "\n",
    "Remember our swiss census from 1880?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_census = pd.read_table(\"data/swiss_census_1880.csv\", sep=\",\")\n",
    "\n",
    "grouped = df_census.groupby(\"canton name\")\n",
    "grouped[\"Catholic\"].sum()                     # number of catholics per canton in 1880."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Micro-Exercise:\n",
    "* Apply the next cell, which makes a copy of the DataFrame and adds a new column named \"Age_category\"\n",
    "* Compute survival rates by Sex, Age_category, and Pclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc = df.copy()\n",
    "def age_category(x):\n",
    "    age_classes = {\"child\": 12, \"teenager\": 17, \"adult\": 64, \"senior\": 200}\n",
    "    for label, threshold in age_classes.items():\n",
    "        if x <= threshold:\n",
    "            return label\n",
    "dfc['Age_category'] = dfc.Age.apply( age_category )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Computing these summary statistics is all good, but you can also vizualize them using `seaborn` argument `hue` : \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot( x='Age', hue='Sex', data=df, kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`hue` is quite ubiquitous in seaborn functions, and lets you determine a categorical column by which you want to split your representation.\n",
    "\n",
    "Note how we switched our syntax too: we now use column names, with a `data=` argument specifying from which `DataFrame` the data comes.\n",
    "\n",
    "<br>\n",
    "\n",
    "The default seaborn scheme is fairly nice, but maybe you want to change it :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots( 2, 2, figsize=(14, 7) )\n",
    "\n",
    "# default\n",
    "sns.kdeplot( x='Age' , hue = 'Sex' , linewidth=3, data=df , ax = axes[0][0])\n",
    "\n",
    "# using an existing seaborn palette\n",
    "sns.set_palette(sns.color_palette('Set2'))\n",
    "sns.kdeplot( x='Age' , hue = 'Sex' , data=df , linewidth=3, ax= axes[0][1])\n",
    "\n",
    "\n",
    "# setting manually, with hex values\n",
    "sns.kdeplot( x='Age' , hue = 'Sex' , data=df , linewidth=3, ax= axes[1][0] , \n",
    "           palette = ['#FFCC04','#1F8AB3'])\n",
    "\n",
    "\n",
    "# setting semi manually, using the xkcd palette https://xkcd.com/color/rgb/\n",
    "sns.kdeplot( x='Age' , hue = 'Sex' , data=df , linewidth=3, ax= axes[1][1] , \n",
    "           palette = [ 'xkcd:lavender' , 'xkcd:dark mint' ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Note:* use `linewidth=` to get wider/narrower lines\n",
    "\n",
    "You can learn more on all the options there :\n",
    " * [matplotlib colors](https://matplotlib.org/stable/tutorials/colors/colors.html)\n",
    " * [seaborn colors](https://seaborn.pydata.org/tutorial/color_palettes.html)\n",
    " \n",
    "\n",
    "<br>\n",
    "\n",
    "Sometimes, the `displot` options don't really cut it, perhaps because you have many diffrent categories, or maybe because you want to have more than one column to do the categorization.\n",
    "\n",
    "This is when **`sns.catplot()`**, another **figure-level** function with a lot a `kind`, saves the day!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot( x='Fare' , y = 'Pclass' ,\n",
    "            orient='horizontal' , \n",
    "            data=df , aspect = 2.0 , height = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like `sns.displot`, `sns.catplot` is a master function which is designed to create a full figure at once rather than be embedded in a larger figure. \n",
    "\n",
    "This is why its arguments are a bit different when it comes to setting the figure *height and width*, which is done using :\n",
    " * `height` : height of the figure (no trick there)\n",
    " * `aspect` : width/height ratio of the figure ( high aspect -> wider figure )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `kind` argument let's you control the overall look of the plot. I\n",
    "It can be:\n",
    "* 'strip' : this is the default, showing all data points. \n",
    "* **'box'** : the famous boxplot\n",
    "* **'violin'** : an alternative to the boxplot using density lines\n",
    "* **'bar'** : the (in)famous barplot\n",
    "* 'swarm' : similar to 'strip' but with another way of aranging the points\n",
    "* 'boxen' : some intermediary between a boxplot and a violin plot\n",
    "* 'point' : alternative to the barplot where only the top point is shown\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kinds = ['box','violin','bar','boxen','strip','point']  #,'swarm'] # swarm takes a long time to compute\n",
    "\n",
    "for i,k in enumerate(kinds):\n",
    "    sns.catplot( x='Fare', y='Pclass', orient='horizontal', data=df, kind=k, aspect=5 , height=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we can see:\n",
    " * **boxplot:** represent quartiles and a few other things\n",
    " * **violinplot:** density line plot with an (optional) boxplot in the center \n",
    " * **barplot:** just shows the mean + an error bar (by default 95% CI computed using bootstrapping)\n",
    " * **boxen:** the unholy child of a boxplot and a violinplot\n",
    " * **strip:** the default, shows the points (super nice, except when you have a lot of points)\n",
    " * **point:** basically a barplot without bars and a line between categories\n",
    "\n",
    "Given all this, one can reasonnably ask the question : **which one is the *best* ?**\n",
    "\n",
    "While the answer is not clear about the best way to represent a distribution over one or multiple categories, we definitely have some [good answers regarding bad/misleading ways](https://stekhoven.shinyapps.io/barplotNonsense/) (hint, barplot does not shine).  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Anyhow the `hue` argument can also be used here to great effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot( x='Fare' , \n",
    "            y = 'Pclass' , \n",
    "            hue='Sex' , \n",
    "            orient='horizontal' , \n",
    "            kind='box', data=df , aspect = 2.0 , height = 4)\n",
    "## it is also very nice with kind='point'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Exercice 3 - representing categories\n",
    "\n",
    "Using the 1880 swiss census data:\n",
    "\n",
    "1. compute a new column \"fraction60+\" representing the fraction of 60+ years old people in each town.\n",
    "    **hint :** column `'60+ y.o.'` contains the number of 60+ years old ; column `'Total'` contains\n",
    "    the total number of inhabitant\n",
    "    \n",
    "2. Represent the proportion of people more 60 years old (`'60+ y.o.'`) across all cantons.\n",
    "   Choose the most appropriate kind of plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_census = pd.read_table(\"data/swiss_census_1880.csv\", sep=\",\")\n",
    "df_census.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 1-6 solutions/solution_01_03.py\n",
    "# 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 7- solutions/solution_01_03.py\n",
    "# 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course possibilities are endless. Here is a fancy one, inspired by [this](https://seaborn.pydata.org/examples/kde_ridgeplot.html) and its [later correction](https://www.pythonfixing.com/2022/02/fixed-python-seaborn-ridge-plot.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_01_03_fancy.py\n",
    "# fancy solution inspired by  https://seaborn.pydata.org/examples/kde_ridgeplot.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "[back to toc](#toc)\n",
    "\n",
    "## 3.4 Representing the relationship between 2 numerical variables <a id=\"descr.4\" ></a>\n",
    "\n",
    "**`sns.scatterplot()`** lets us represent the relationship between two numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(\"data/titanic.csv\", sep=\",\") ## just in case we need to re-read  the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset the seaborn default theme\n",
    "sns.reset_orig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14,6))\n",
    "ax = sns.scatterplot(x='Fare', y='Age', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, it is not so nice... let's set the x-axis to a log scale :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14,6))\n",
    "ax = sns.scatterplot( x = 'Fare' , y = 'Age' , data=df )\n",
    "ax.set(xscale=\"log\")      # setting axis to log scale.\n",
    "ax.set_xlim( (3,1000) )   # manually setting the limit of the x axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can customize :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = sns.scatterplot( x = 'Fare' , y = 'Age' , \n",
    "                     hue='Pclass' , \n",
    "                     palette=['xkcd:tomato',\n",
    "                              'xkcd:teal','xkcd:mustard'],\n",
    "                     style='Sex', data=df , s=100 )\n",
    "ax.set(xscale=\"log\")      # setting axis to log scale.\n",
    "ax.set_xlim( (3,1000) )   # manually setting the limit of the x axis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Note:* `style=` associates the marker shape to a column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a little data exploration trick : **`sns.pairplot()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_census.iloc[:,3:7] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "[back to toc](#toc)\n",
    "\n",
    "\n",
    "# 4. Writing data and plots to disk <a id='writing'></a>\n",
    "-------------------------------------------------\n",
    "\n",
    "To write your data-set to a file, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('myData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it is about as simple as reading. Actually, it is really similar.\n",
    "You have several functions :\n",
    "* `to_csv()`: write DataFrame as comma-separated file, or any other separator-delimited format such as\n",
    "  tab-delimited. \n",
    "* `to_excel()`: write DataFrame in Excel format.\n",
    "* `to_html()`: write DataFrame in HTML format.\n",
    "* More [writer functions are available](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html)\n",
    "\n",
    "<br>\n",
    "\n",
    "... and similar arguments : when using the `to_csv()` writer functions, some useful arguments are:\n",
    "* `sep`: the type of delimiter to use. By default, `sep=\",\"`. To write a tab-delimited file e.g., one would\n",
    "  set `sep=\"\\t\"`.\n",
    "* `header=None` to not include the header in the exported file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "To save figures to disk, we rely on the **`savefig()`** method:\n",
    "* Different output formats can be specified by changing the output file name extension, \n",
    "  e.g. `.pdf`, `.svg`, `.jpg`, ...\n",
    "* For raster formats, the `dpi` argument can help define the image quality.\n",
    "* See `help(plt.savefig)` or [here](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.savefig.html?highlight=savefig#matplotlib.pyplot.savefig) for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k='box'\n",
    "\n",
    "# Make you plot as usual, redirecting it to a variable (my_plot)\n",
    "represented_variable , category = 'Total' , 'majority language'\n",
    "my_plot = sns.catplot( x = represented_variable , y= category ,\n",
    "             data=dfFractions , kind = k , orient='h',height=10, aspect=2 )\n",
    "\n",
    "# Save plot to disk, using the savefig() method:\n",
    "my_plot.savefig(\"output.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(plt.savefig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For multipanel figures:\n",
    "f, axes = plt.subplots(2, 2, figsize=(7, 7))\n",
    "\n",
    "sns.histplot(dfFractions['0-14 y.o.'], kde=False, color=\"b\", ax=axes[0, 0]).set_title('automatic')\n",
    "sns.histplot(dfFractions['0-14 y.o.'], bins=5   , kde=False, color=\"b\", ax=axes[0, 1]).set_title('5 bins')\n",
    "sns.histplot(dfFractions['0-14 y.o.'], bins=10  , kde=False, color=\"b\", ax=axes[1, 0]).set_title('10 bins')\n",
    "sns.histplot(dfFractions['0-14 y.o.'], bins=1000 , kde=False, color=\"b\", ax=axes[1, 1]).set_title('1000 bins')\n",
    "plt.tight_layout()# this makes the panels margins and setup more graceful\n",
    "\n",
    "f.savefig('output_multipanel.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "[back to toc](#toc)\n",
    "\n",
    "# 5. Free form exercise <a id='exo'></a>\n",
    "---------------------------------\n",
    "\n",
    "The goal of this exercise is to perform an exploration of some data related to heart disease.\n",
    "\n",
    "In particular, we want to explore the relationship between a `target` variable - whether patient has a heart disease or not - and several other variables such as cholesterol level, age, ...\n",
    "\n",
    "The data is present in the file `'data/heartData_simplified.csv'`, which is a cleaned and simplified version of the [UCI heart disease data set](https://archive.ics.uci.edu/ml/datasets/heart+Disease)\n",
    "\n",
    "\n",
    "### Description of the columns\n",
    "\n",
    "* age : Patient age in years\n",
    "* sex : Patient sex\n",
    "* chol : Cholesterol level in mg/dl. \n",
    "* thalach : Maxium heart rate during the stress test\n",
    "* oldpeak : Decrease of the ST segment during exercise according to the same one on rest.\n",
    "* ca : Number of main blood vessels coloured by the radioactive dye. The number varies between 0 to 3.\n",
    "* thal : Results of the blood flow observed via the radioactive dye.\n",
    "\t* defect -> fixed defect (no blood flow in some part of the heart)\n",
    "\t* normal -> normal blood flow\n",
    "\t* reversible -> reversible defect (a blood flow is observed but it is not normal)\n",
    "* target : Whether the patient has a heart disease or not\n",
    "\n",
    "### Instructions\n",
    "\n",
    "As stated earlier, your goal is to explore this data-set. \n",
    "One objective of this would be to diagnose eventual problems in this dataset (outliers, strange values) and prepare further statistical analysis and reporting.\n",
    "\n",
    "To this end you will want to formulate a number of hypothesis that would be interesting to pursue from this data (*e.g.*, is heart disease linked to  cholesterol levels), and gather evidence (plots, summary statistics) explaining why this hypothesis seems to be worth testing for.\n",
    "\n",
    "> Note : we do not ask you to perform the statistical testing itself. Do it if you feel like it.\n",
    "\n",
    "We will not provide a particular set of precise questions, but here are a few checkpoints to help you get stared :\n",
    "\n",
    "* Read the data as a pandas `DataFrame`\n",
    "* Compute summary statistics for the different variables \n",
    "* eventually, do the same for different subset of the data (for instance, grouping by sex)\n",
    "* use visualization to help you describe the relationship between the different variables\n",
    "* choose a few associations (2?4?) that seems promising and describe them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "[back to toc](#toc)\n",
    "\n",
    "\n",
    "# 6. tips and tricks <a id='tricks'></a>\n",
    "---------------------------\n",
    "\n",
    "This is a collection of nice tricks that don't really blong anywhere but are nice to know"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 6.1 Applying custom functions to a DataFrame by rows or columns\n",
    "\n",
    "As we have just seen, pandas DataFrame have a number of built-in methods - e.g. `describe()`, `count()`, `mean()` - that can be applied row-wise or column-wise.\n",
    "\n",
    "But it is also possible to apply any custom function on rows/columns using `apply(func, axis)`:\n",
    "* `func`: the function to apply.\n",
    "* `axis=0` to apply the function row-wise (this is the default), or `axis=1` to apply it column-wise.\n",
    "\n",
    "For example, let's implement our own version of the built-in [`mean()` function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mean.html).\n",
    "\n",
    "**Note:** the `apply()` function only works on DataFrames. To apply a custom function to each element in a Series \n",
    "(e.g. a single column of a DataFrame), the `map()` function can be used. E.g.: `df[\"Survived\"].map(func)`\n",
    "\n",
    "<br>\n",
    "\n",
    "**Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the DataFrame as a reminder:\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_str(seq):\n",
    "    max_len = -1\n",
    "    max_value = ''\n",
    "    for x in seq:\n",
    "        length = len(str(x))\n",
    "        if length > max_len:\n",
    "            max_len = length\n",
    "            max_value = x\n",
    "    return max_value\n",
    "\n",
    "# Test of the longest_str() function on dummy data:\n",
    "print(\"The longest string is:\", longest_str((\"eggs\", \"hello world\", \"helloworld\", \"spam\")), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply custom function by column (i.e row with the most characters in each column):\n",
    "print(df.apply(longest_str, axis=0), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply custom function by row (i.e column with the most characters in each row):\n",
    "print(df.apply(longest_str, axis=1), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 6.2 Sorting operations on dataframes\n",
    "\n",
    "DataFrames can be sorted on one or more specific column(s) using **`sort_values()`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sc = df_sc.sort_values(df_sc.columns[0], ascending=False)\n",
    "df_sc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Or we can sort by index using **`sort_index()`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_sc.sort_index(ascending=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the maximum value in each column. Note the `axis` parameter. This gives the dimension along which values are compared. `axis=0` indicates that the comparison is across rows and there looping over all index values in all other dimensions. In this case, for each column we got a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sc.max(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now find the max in each row (in this case for each gene), over all columns (samples):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_sc.max(axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe what we want is not the maximum value but the index at which it is found: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sc.idxmax(axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 6.4 Merge and join DataFrames\n",
    "\n",
    "The **[`merge()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html)** and **[`join()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.join.html)** methods allow to combine DataFrames, linking their rows based on a common column (other referred to as a **key**).\n",
    "\n",
    "To illustrate these 2 methods, let's create 2 DataFrames that we can merge.\n",
    "> *Note:* this also illustrates how **a dataframe can be constructed from a dictionary** data structure.\n",
    "  The dictionary keys are treated as column names, and the list of values associated with a key is\n",
    "  treated as list of elements in the corresponding column. Note that all columns should have the same\n",
    "  number of elements (or a single element, in which case all rows of the column contain this same\n",
    "  element).  \n",
    "  If no index is specified, pandas uses its default indexing, i.e. row positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\n",
    "    'key': ['b','b','a','c','a','a','b'], \n",
    "    'data1': range(7)\n",
    "})\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({\n",
    "    'key': ['a','b','d'], \n",
    "    'data2': range(3)\n",
    "})\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's merge the two data frames, with the default application of the `merge()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How has python interpreted our call to `merge()`?\n",
    "\n",
    "1. It has assumed that we want to merge on the basis of the common `key` column.\n",
    "2. It has identified the values of `key` which occur in both dataframes.\n",
    "3. It has generated a dataframe with all combinations of rows from dataframes 1 and 2 that are \n",
    "   associated with a particular `key` value.\n",
    "\n",
    "We can be more precise by specifying how to merge the dataframes, using the **`on`** option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df2, on=\"key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, merge performs an \"inner\" operation, taking the intersection of the key sets. However, we can specify the way we want to merge by passing `\"outer\"`, `\"left\"`, `\"right\"` to the **`how`** argument. This determines which set of keys to consider (the union of the two sets, all of those that occur in df1, all of those that occur in df2). Missing values show up as `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df2, on=\"key\", how=\"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging can also be done based on the index values. Let's illustrate this using the mouse heart gene expression dataset we used earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_table(\"data/heartData_simplified.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create two data frames, one containing the data for the WT and the other for the KO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_cols = [col for col in df.columns if 'WT' in col and 'avg' not in col and 'log' not in col]\n",
    "ko_cols = [col for col in df.columns if 'KO' in col and 'avg' not in col and 'log' not in col]\n",
    "df_WT = df[wt_cols]\n",
    "df_KO = df[ko_cols]\n",
    "print(df_WT.head())\n",
    "print(df_KO.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's merge these frames based on the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(df_WT, df_KO, left_index=True, right_index=True)\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 6.5 wide and long format\n",
    "\n",
    "**Wide :**\n",
    "\n",
    "| Id     | mol1   | mol2    |\n",
    "| ------ |:------:| -------:|\n",
    "| a      | 1.0    | 10.0    |\n",
    "| b      | 2.0    |   20.0  |\n",
    "| c      | 3.0    |    30.0 |\n",
    "\n",
    "**Long :**\n",
    "\n",
    "| Id     | Value  | Attr    |\n",
    "| ------ |:------:| -------:|\n",
    "| a      | 1.0    | mol1    |\n",
    "| a      | 10.0   | mol2    |\n",
    "| b      | 2.0    | mol1    |\n",
    "| b      | 20.0   | mol2    |\n",
    "| c      | 3.0    | mol1    |\n",
    "| c      | 30.0   | mol2    |\n",
    "\n",
    "\n",
    "Sometimes it can be interesting to go from wide to long or long to wide, because some operations are easier on one format or the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data in wide format:\n",
    "df_sc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sc[\"gene\"] = df_sc.index\n",
    "df_sc_long = pd.melt(df_sc, id_vars=['gene']) # to long format, with gene as identifiers \n",
    "df_sc_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This now allows us to do this, which was a bit difficult otherwise :\n",
    "df_sc_long['logVal'] = np.log10(10**0 + df_sc_long['value'])\n",
    "\n",
    "g = sns.catplot( x='logVal' , y='variable' , orient='horizontal' , data=df_sc_long , \n",
    "                kind='bar',\n",
    "                 aspect = 2 , height = 7 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 seaborn themes + python graph gallery\n",
    "\n",
    "Seaborn and matplotlib offer nigh-endless possibilities when it comes to costumizing graph elements.\n",
    "\n",
    "A simple way to change the general feeling of a figure is to play with [seaborn themes](https://seaborn.pydata.org/tutorial/aesthetics.html), or [matplotlib styles](https://www.dunderdata.com/blog/view-all-available-matplotlib-styles).\n",
    "\n",
    "In general, when composing a figure we often do not start from nothing, and work our ways from existing examples.\n",
    "Here are useful resources to do just that:\n",
    "- [matplotlib gallery](https://matplotlib.org/stable/gallery/index.html)\n",
    "- [seaborn gallery](https://seaborn.pydata.org/examples/index.html)\n",
    "- [python-graph-gallery](https://www.python-graph-gallery.com/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_py38)",
   "language": "python",
   "name": "conda_py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
