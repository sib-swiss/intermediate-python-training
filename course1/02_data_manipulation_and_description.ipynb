{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1 - Working with tabulated data \n",
    "----------------------------------------------------------------\n",
    "\n",
    "## Table of Content <a id='toc' />\n",
    "\n",
    "1. **[DataFrame subsetting](#3)**  \n",
    "    1.1 [Accessing specific rows and columns](#31)  \n",
    "    1.2 [Conditional selection](#32)  \n",
    "    <br>\n",
    "    \n",
    "2. **[Operations on columns](#4)**  \n",
    "    2.1 [Arithmetic operations](#41)  \n",
    "    2.2 [Applying built-in summary functions](#42)  \n",
    "    2.3 [Applying custom functions](#43)  \n",
    "    <br>\n",
    "\n",
    "3. **[Grouping data by factor](#a3)**\n",
    "\n",
    "4. **[Writing DataFrames to disk](#5)**\n",
    "   <br>\n",
    "\n",
    "**Supplementary Material - [Additional topics, not covered in class](#a)**  \n",
    "  * Annex 1 - [Sorting operations on dataframes](#a1)  \n",
    "  * Annex 2 - [Dropping rows with missing values](#a2)  \n",
    "  \n",
    "  * Annex 4 - [Creating DataFrames and Series from scratch](#a4)  \n",
    "  * Annex 5 - [Concatenate, merge and join DataFrames](#a5)  \n",
    "  * Annex 6 - [Mixed selection by names and positions for `.loc[]` and `.iloc[]`](#a6)  \n",
    "  * Annex 7 - [more ways to add rows to a DataFrame](#a7)\n",
    "  * Annex 8 - [Wide and long format](#a8)  \n",
    "  \n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. DataFrame subsetting <a id='3'></a>\n",
    "\n",
    "## 1.1 Accessing specific rows and columns <a id='31'></a>\n",
    "\n",
    "A very common operation to perform on *DataFrames* is to **create a subset** by selecting certain rows and/or columns.\n",
    "\n",
    "There are 2 methods in pandas to perform a selection on a DataFrame (here `df`):\n",
    "* **`df.loc[<row index values>, <column names>]`** - to select based on row (index) and column **names**.\n",
    "* **`df.iloc[<row positions>, <column positions>]`** to select based on row and column **positions**.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Subsetting a DataFrame with the `.loc[]` indexer\n",
    "\n",
    "The **`.loc[]` indexer** selects rows and columns based on the **index values of the row** and the **names of the columns**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Re)Load the titanic dataset as a DataFrame.\n",
    "df = pd.read_csv(\"data/titanic.csv\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[2, \"Name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Multiple rows/columns** can be selected by:\n",
    "* **Passing a sequence** (e.g. `list`, `tuple`) of row/column names: **`.loc[[0, 10], [\"Name\", \"Age\"]]`**.\n",
    "* **Passing a slice** of row/column names: **`.loc[0:10, \"Name\":\"Age\"]`**.\n",
    "  * **`value:`** selects all rows/columns **from `value` till the end**.\n",
    "  * **`:value`** selects all rows/columns **from the start until `value` (included)**.\n",
    "  * **`:`** with no values around **selects all rows/columns**.\n",
    "  \n",
    "    > *Note:* when selecting on rows only (i.e. select all columns), the `df.loc[<row selection>, ]`\n",
    "    > and `df.loc[<row selection>]` syntaxes are also possible. The `:` is not compulsory in that case.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "**Important:** when using *slicing*, the **end element of the slice is included** - unlike every other time in python !\n",
    "\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "* **Select the first 3 rows** of the columns `Name`, `Age` and `Pclass`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0:2, [\"Name\", \"Age\", \"Pclass\"]]\n",
    "df.loc[:2, [\"Name\", \"Age\", \"Pclass\"]]   # Same as above, but omitting the \"0\" since it's the first index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* **Select a range** of columns using *slicing*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:2, \"Name\":\"Pclass\"]  # Reminder: with `.loc[]`, the end element of the slice is included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "A column can be selected multiple time, in any order. This can be used to **re-order columns**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting all rows, re-arranging columns, and duplicating the \"Age\" column.\n",
    "df.loc[:, [\"Age\", \"Pclass\", \"Name\", \"Age\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Selecting a single row/column** returns a pandas **Series** object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df.loc[2, ]))\n",
    "df.loc[2, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Pandas Series**  \n",
    "> *Series* are the equivalent of *DataFrame*, but **1-dimensional** (so essentially they\n",
    "> are a **named vector** of values).  \n",
    "> \n",
    "> Their elements can be accessed in quite a similar way:\n",
    ">\n",
    ">    ```py\n",
    ">    row_5 = df.loc[4,:]\n",
    ">    print(row_4[0])      # Access an item of a Series by position.\n",
    ">    print(row_4.Age)     # Access an item of a Series by name.\n",
    ">    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Pitfalls with `.loc[]`\n",
    "\n",
    "**Because the index values often correspond to row positions** (as in our example DataFrame), it is easy to get the wrong impression that `.loc[]` also selects on row positions... but this is not the case.\n",
    "\n",
    "To illustrate this, let's load a DataFrame with **non-numeric Index values**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the titanic data set with the \"Name\" column as index.\n",
    "tmp = pd.read_csv(\"data/titanic.csv\", index_col=\"Name\")\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Let's now try to **select rows 2-5** of our *DataFrame*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp.loc[1:4, :]  # Oops... this raises a TypeError."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "With `.loc[]`, **we must pass Index/column names**. Positions are not allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all rows from \"Coleff Mr. Peju\" to \"Dooley Mr. Patrick\" (included):\n",
    "tmp.loc[\"Coleff Mr. Peju\":\"Dooley Mr. Patrick\", :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸŒˆ **Note:** you could get around this problem by querying the index at the desired positions:\n",
    ">\n",
    "> ```python\n",
    "> tmp.loc[tmp.index[1]:tmp.index[4], :]\n",
    "> ```\n",
    "> <br>\n",
    ">\n",
    "> But at this point it's a lot more convenient to **use the `.iloc[]` indexer**\n",
    "> (see the supplementary material section below for details):\n",
    ">\n",
    "> ```python\n",
    "> tmp.iloc[1:5, :]\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### Supplementary material: the `.iloc[]` indexer\n",
    "\n",
    "The **`.iloc[]` indexer** is very similar to `.loc[]`, except that it selects based on row/column position rather than on index and column names.\n",
    "* As usual in Python, **position indexes are zero based**, meaning that the first row/column has position 0.\n",
    "* Unlike `.loc[]`, with **`.iloc[]`, the end index is excluded**, as usual when doing slicing in Python.\n",
    "\n",
    "  ```python\n",
    "  df.iloc[0:3, :]  # Selects the first 3 rows.\n",
    "  df.loc[0:3, :]   # Selects the first 4 rows.\n",
    "  ```\n",
    "\n",
    "<br>\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "```python\n",
    "# Select the first 3 rows (positions 0 to 2).\n",
    "df.iloc[0:3, :]\n",
    "\n",
    "# Select the 6th, 7th and 8th rows (positions 5 to 7) and the last 2 columns.\n",
    "df.iloc[5:8, -2:]\n",
    "```\n",
    "\n",
    "    \n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "### Micro-Exercise 1\n",
    "\n",
    "Load the `data/titanic.csv` dataset with: `df = pd.read_csv(\"data/titanic.csv\")`.\n",
    "\n",
    "Then, using the **`.loc[]`** indexer:\n",
    "* **Select** all rows with odd Index values from the Titanic data frame, and the columns `Name`, `Age` and `Fare`.\n",
    "* **Re-order** the columns so that `Age` is first and `Name` is second.\n",
    "* ðŸŽ¯ **Hint:** we will soon see how to perform conditional selections, but for now you can use the `range()`\n",
    "  function to help you with this task.  \n",
    "  Example: `range(0, 21, 3)`  ->  `[0, 3, 6, 9, 12, 15, 18]`\n",
    "\n",
    "<br>\n",
    "\n",
    "ðŸ”® **Additional task, if you have time:**\n",
    "* Perform the same operation, but using the `.iloc[]` indexer. See the supplementary material above for how to use `.iloc[]`.\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "[Back to ToC](#toc)\n",
    "\n",
    "## 1.2 Conditional selection <a id='32' />\n",
    "\n",
    "A powerful way of selecting rows of a DataFrame is by **passing a sequence of boolean values as row selection** to the `.loc[]` indexer. This allows to **filter via conditional selection**.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "**Important:** Making **conditional selections is only supported by the `.loc[]`** indexer. It does *not* work with `.iloc[]`.\n",
    "\n",
    "> As a workaround for **`.iloc`**, one can call the index of a mask, or use the\n",
    "  [query method](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html).\n",
    "  E.g. `df.query(\"Fare > 300\")`.\n",
    "\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "**Example:** In the Titanic dataset, select all passengers older than 50 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/titanic.csv\")\n",
    "\n",
    "df.loc[df.Age > 50, :]\n",
    "#df.iloc[df.Age > 50, :]  # Raises a NotImplementedError."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Conditional selection** is frequently done by first creating a boolean **mask**, which is then applied to filter the DataFrame:\n",
    "* A **mask** is a vector of boolean values (`True`/`False`) that indicate whether\n",
    "  or not a rows satisfies to the defined condition.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Example:** crate a masks that filters for women in the Titanic dataset. Here the mask values are `True` if the corresponding value in the `Sex` column of the DataFrame is equal to `female`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mask = df[\"Sex\"] == \"female\"   # This creates a vector (a pandas Series) of boolean values (True/False).\n",
    "mask.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now that the mask is created, we can use it to **filter our `DataFrame`** and keep only those rows corresponding to female passengers.\n",
    "\n",
    "> ðŸ¦‰ **Reminder:** `.shape[0]` returns the number of rows of a *DataFrame*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of female passengers:\n",
    "df.loc[mask,].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[mask, \"Name\":\"Pclass\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Combining conditions\n",
    "\n",
    "Selection conditions can be combined to produce more complex selection criteria.  \n",
    "* Conditions are combined using the **`&`** (logical AND) and **`|`** (logical OR) operators.\n",
    "* ðŸ”¥ **Important:** each conditions must be surrounded by brackets: `( condition 1 ) & ( condition 2 )`.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "* Select men passengers with a fare > 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df.Sex == \"male\") & (df.Fare > 200), ].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* Same as above, but in 2 steps: first we create a mask, then we apply it to the *DataFrame*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df.Sex == \"male\") & (df.Fare > 200) \n",
    "df.loc[mask, ].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* Select all people that are **either < 25 or women**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df.Age < 25) | (df.Sex == \"female\"), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Conditional value assignment\n",
    "\n",
    "**Row value assignment operations can be combined with a selection operation**, allowing e.g. to modify the values of certain rows in a DataFrame based on a given condition.\n",
    "\n",
    "Let's imagine that, for some reason, the fares of class 3 passengers are not valid. We want to set them to `NA` values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()  # Create a copy of the DataFrame, to avoid modifying the original one.\n",
    "\n",
    "# NA is represented using pd.NA\n",
    "df_copy.loc[df.Pclass==3, 'Fare'] = pd.NA\n",
    "df_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "### Micro-Exercise 2\n",
    "\n",
    "* From the Titanic dataset, create a mask to select passengers in first class (`Pclass` is `1`)\n",
    "  that are less than 18 years old.\n",
    "* What fraction of these passengers survived?  \n",
    "  ðŸŽ¯ **Hint:** apply the `.mean()` method to a column to compute its mean value.\n",
    "\n",
    "<br>\n",
    "\n",
    "ðŸ”® **Additional Task, if you have time:**\n",
    "* Create a mask to select women and children (< 18 years). How many were onboard the ship?\n",
    "* Compute the median ticket price (`Fare` column) and age for men and women.\n",
    "  Use a `for` loop to avoid code duplication (alternatively, you could look into the `.groupby()` method).\n",
    "\n",
    "<div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "### Additional material: copy or not copy?\n",
    "\n",
    "What happens if we select a subset of a DataFrame and modify it? Does the original data stay the same?\n",
    "\n",
    "This issue is quite complex, but as you are likely to encounter this warning at some point, we here provide a short primer on it. The following link also provides a more [in-depth explanation of view vs. copy](https://www.dataquest.io/blog/settingwithcopywarning).\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(\"data/titanic.csv\", sep=\",\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.Sex == \"male\", \"Age\"] = 999\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "In the above example, pandas changes the values of the `Age` column in the original `DataFrame` object.\n",
    "\n",
    "In general, `pandas` avoids doing copies when it can... but let's try another example:\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the subset to a new variable.\n",
    "df_male = df.loc[df.Sex == \"male\", :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the age to 888 in the subset dataframe of males:\n",
    "df_male.Age = 888"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "What's this? **We get a warning!**\n",
    "* As can be seen below, the `Age` value for males has been modified in the `df_male` DataFrame, but not in the\n",
    "  original `df` DataFrame.\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_male.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "In this case, the change is made to `df_male` only and not to the original DataFrame `df`.\n",
    "\n",
    "Sadly, it is not easy to know when you get a **view** or a **copy**.\n",
    "\n",
    "<br>\n",
    "\n",
    "![image.png](img/view_copy.png)\n",
    "\n",
    "<br>\n",
    "\n",
    " * **View:** pointer to the original DataFrame (or to a subset of it).\n",
    " * **Copy:** new DataFrame object (data is physically copied in memory).\n",
    "   Modifying a copy leaves the original data untouched.\n",
    "\n",
    "In general, **using `.loc[]` should return a view**, however that also depends on the evaluation order of some of the performed operations.\n",
    "\n",
    "<br>\n",
    "\n",
    "When you intend to make a copy, it is recommended to explicitly use the **`copy()`** method of DataFrame.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Explicitly create a copy of the DataFrame returned by .loc[]\n",
    "df_male = df.loc[df.Sex == 'male', :].copy()\n",
    "\n",
    "# Now we don't get a warning anymore:\n",
    "df_male.Age = 888\n",
    "df_male.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "[Back to ToC](#toc)\n",
    "\n",
    "# 2. Operations on columns <a id='4' />\n",
    "\n",
    "## 2.1 Arithmetic and logical operations <a id='41' />\n",
    "\n",
    "A great strength of pandas DataFrame is that it allows using **arithmetic operators directly on columns**.  \n",
    "And as we have seen when doing conditional selection, this also works with **logical operators**, such as `>`, `<`, `==`, or `!=`.\n",
    "\n",
    "> ðŸŒˆ **Note:** the usual shortcuts to increment/decrement a variable also apply here.\n",
    ">\n",
    ">   ```python\n",
    ">   df[\"column\"] += x  # Increment the column by `x`.\n",
    ">   df[\"column\"] -= x  # Decrement the column by `x`.\n",
    ">   df[\"column\"] /= x  # Divide the column by `x`.\n",
    ">   df[\"column\"] *= x  # Multiply the column by `x`.\n",
    ">   ```\n",
    "\n",
    "<br>\n",
    "\n",
    "**Examples**:\n",
    "\n",
    "* In the Titanic dataset, **increase the age of all passengers by 1 year**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the DataFrame for reference.\n",
    "df = pd.read_csv(\"data/titanic.csv\").dropna()\n",
    "df.Age = df.Age.astype(int)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase everyone's age by 1 year.\n",
    "df.Age = df.Age + 1\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Examples using the Swiss 1880 census dataset:**\n",
    "* The `Total` column gives the total number of registered inhabitants in a given town.\n",
    "* The `Female` columns gives the number of women in a given town."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Swiss 1880 census dataset.\n",
    "df_census = pd.read_csv(\"data/swiss_census_1880.csv\")\n",
    "df_census.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* To compute the **fraction of women in each town**, we can simply write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_census.Female / df_census.Total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* Better yet - we can very easily assign our result to a **new column**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_census[\"Female_Fraction\"] = df_census.Female / df_census.Total\n",
    "\n",
    "# Display DataFrame subset of interest.\n",
    "df_census.loc[:, [\"town name\", \"Total\", \"Female\", \"Female_Fraction\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* We can test whether the dataset is consistent: the total number of women + men should\n",
    "  equal the total population of a town.\n",
    "\n",
    "> âœ¨ **Tip:** you can **invert a mask** (or any vector of boolean values) by prefixing it with **`~`**:\n",
    ">\n",
    ">    ```py\n",
    ">    df.loc[~mask, :]\n",
    ">    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df_census.Male + df_census.Female == df_census.Total\n",
    "\n",
    "# Display the rows with errors, where the number of women + men does not match the total population.\n",
    "df_census.loc[~mask, [\"town name\", \"Total\", \"Male\", \"Female\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "### Micro-Exercise 3\n",
    "\n",
    "Reload the Titanic dataset with `df = pd.read_csv(\"data/titanic.csv\")`, and perform the following task:\n",
    "   \n",
    "* Children under the age of 10 get a special discount of 50% on their fare.\n",
    "  Apply this by dividing by 2 the `Fare` of eligible passenger in the `df` DataFrame.\n",
    "\n",
    "<br>\n",
    "\n",
    "ðŸ”® **Additional task, if you have time:**\n",
    "* Load the Swiss census 1880 dataset: `df_census = pd.read_csv(\"data/swiss_census_1880.csv\")`.\n",
    "* Create a mask to filter the dataset for towns that have a majority of Italian or Romansh speakers.\n",
    "* In which cantons are these towns located, and how many are in each canton?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "[Back to ToC](#toc)\n",
    "\n",
    "## 2.2 Applying built-in summary functions <a id='42' />\n",
    "\n",
    "The pandas `DataFrame` and `Series` objects (reminder: a single column of a `DataFrame` is a `Series`) have a number of **built-in methods to compute summary statistics on a per column (or per row) basis**.\n",
    "\n",
    "* **`.count()`**: number of non-NA values.\n",
    "* **`.value_counts()`**: count the number of occurrences of each value (works better with categorical data).\n",
    "* **`.sum()`**, **`.mean()`**, **`.max()`**, **`.min()`**: sum/mean/max/min values.\n",
    "* **`.std()`**, **`.var()`**: standard deviation and variance values.\n",
    "* **`.round()`**: rounds values to the specified decimal.\n",
    "* **`.all()`/`.any()`**: returns `True` if all elements/at least one element are `True` (truthy), `False` otherwise.\n",
    "* **`.describe()`**: provide a summary of different statistics (mean, median, max, min, etc...).\n",
    "* ... and more\n",
    "\n",
    "**By default, these statistics are computed per column (`axis=0`).** To compute them per row, the `axis=1` argument must be passed.\n",
    "* `axis=0`: apply the operation on columns (this is the default value).\n",
    "* `axis=1`: apply the operation on rows.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "**Examples:**\n",
    "* Compute the mean of an individual column (pandas `Series`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/titanic.csv\")\n",
    "\n",
    "# Average passenger age:\n",
    "df[\"Age\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* Get the number of passengers in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Pclass\"]].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* Retrieve the maximum value of the `Age` and `Fare` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Age\", \"Fare\"]].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Sometimes what we want is not the maximum/minimum value itself but **the index at which the maximum/minimum value is found** (e.g. to retrieve the entire row corresponding to that max/min value).  We can get this using:\n",
    "* **`.idxmax()`** to retrieve the index of the row with the maximum value.\n",
    "* **`.idxmin()`** to retrieve the index of the row with the minimum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the row of the passenger that paid the highest ticket price.\n",
    "print(\"Index of row with max value of 'Fare':\", df.Fare.idxmax())\n",
    "\n",
    "df.loc[[df.Fare.idxmax()], ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> âœ¨ **Tip:** to prevent the selection of a single row to be converted to a *Series*, we can pass the\n",
    "> single element we select as a list with one element (in this case `[df.Fare.idxmax()]` instead\n",
    "> of `df.Fare.idxmax()`).\n",
    "> \n",
    "> ```python\n",
    "> df.loc[[df.Fare.idxmax()], ]\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "A very useful method of DataFrame to get an **overview of a dataset** is **`df.describe()`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The `df.describe()` method gives information about all numerical columns in the dataset at once\n",
    "  (note that by default, non-numerical columns are absent).\n",
    "* It is very useful not only to get a first impression on the dataset, but also to catch eventual errors\n",
    "  in the data: a negative number where there should be only positive values, missing values (NAs), ...\n",
    "\n",
    "\n",
    "> *Note:* by default, `.describe()` only gives a summary of **numeric columns**.\n",
    "> To include all columns in the summary table:\n",
    ">\n",
    "> ```py\n",
    "> df.describe(include=\"all\")\n",
    "> ```\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Anyway, back to the matter at hand.\n",
    "\n",
    "**`.describe()`** gives access to some of the most commonly used summary statistics:\n",
    "* (arithmetic) **mean**: ${\\displaystyle \\bar{x}={\\frac {1}{n}}\\sum _{i=1}^{n}x_{i}}$  or,\n",
    "  for coders: `sum(x) / len(x)`\n",
    "* **standard deviation** (a.k.a. std, stdev): this corresponds to the average of the absolute difference to the mean. It is the **square root of the variance**.\n",
    "* **minimum** and **maximum**: smallest and biggest value among the data. Looking at them can help detect outliers.\n",
    "* **quartiles**: they correspond to the value such that\n",
    "    * 25% (first quartile, Q1), \n",
    "    * 50% (second quartile, Q2, median), or\n",
    "    * 75% (second quartile, Q3)\n",
    "    \n",
    "  of the values are lower than them. They are less sensitive than the mean to outlier values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "[Back to ToC](#toc)\n",
    "\n",
    "## 2.3 Applying custom functions <a id='43'/>\n",
    "\n",
    "### Apply a custom function to each elements of a DataFrame or a Series (column of a DataFrame)\n",
    "\n",
    "* **`.map(<function to apply>)`**: applies a given function to **each element** of a `DataFrame`/`Series`.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* Let's start by creating a custom function. **It is important that this function takes exactly 1 argument** (to be specific, no more than 1 positional argument, additional optional keyword arguments can be present)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Declare a custom function the computes the log10 of a value.\n",
    "def log10(x):\n",
    "    \"\"\"Return the log10 of the input value.\"\"\"\n",
    "    return 0 if x <= 0 else math.log10(x)\n",
    "\n",
    "# Test run of our custom function.\n",
    "for x in (0, 1, 10, 100, 1000):\n",
    "    print(f\"{x} -> {log10(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* We now **apply our `log10` function to (each element of) the `Fare` column** in the Titanic dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/titanic.csv\")\n",
    "\n",
    "df[\"Log10_fare\"] = df.Fare.map(log10)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **`.map()` also works on entire DataFrames**: it will apply the custom function to each element\n",
    "  of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[[\"Age\", \"Fare\", \"Pclass\"]].map(log10).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "> ðŸŒˆ **Note:** if the custom function we want to pass is short and simple, we can write it as a\n",
    "> **lambda expression** (i.e. anonymous function) rather than defining a function for it.\n",
    ">\n",
    ">   ```python\n",
    ">   # The same log10 function as we used above.\n",
    ">   df[[\"Age\", \"Fare\", \"Pclass\"]].map(lambda x: 0 if x <= 0 else math.log10(x))\n",
    ">\n",
    ">   # A simple function that computes the square of a value.\n",
    ">   df[[\"Age\", \"Fare\", \"Pclass\"]].map(lambda x: x**2)\n",
    ">   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "\n",
    "### Additional material: applying custom functions to a column as a whole (function takes entire column as input)\n",
    "\n",
    "* **`.apply()`**: applies a custom function to the columns of a DataFrame as a whole.\n",
    "* The custom function must accept a single positional argument that expects a **sequence of values**.\n",
    "* **Warning:** the `.apply()` method also exists for `Series`, but it applies the specified\n",
    "  function to each element of the Series. In other words, it behaves just like `.map()` does on DataFrame.\n",
    "\n",
    "\n",
    "**Example:**\n",
    "    \n",
    "```python\n",
    "def sum_of_squares(values):\n",
    "    \"\"\"Computes the sum of squares of a sequence of values.\"\"\"\n",
    "    return sum([x**2 for x in values])\n",
    "\n",
    "\n",
    "df_subset = df.dropna()                                     # Create a new DataFrame with no NA values.\n",
    "df_subset[[\"Age\", \"Fare\", \"Pclass\"]].apply(sum_of_squares)  # Apply custom function by column.\n",
    "\n",
    "# Returns the following Series:\n",
    "#  Age       7.743160e+05\n",
    "#  Fare      2.843380e+06\n",
    "#  Pclass    4.071000e+03\n",
    "#  dtype: float64\n",
    "```\n",
    "\n",
    "* Functions can also be applied to **by row** by passing **`axis=1`** (the function is applied to the row as a whole).\n",
    "\n",
    "```python\n",
    "df_subset[[\"Age\", \"Fare\", \"Pclass\"]].apply(sum_of_squares, axis=1) \n",
    "```\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "### Important: vectorization vs. loops\n",
    "\n",
    "When processing data in a `DataFrame`, you should **avoid using loops** as much as possible.  \n",
    "Instead, use the pandas vectorization functions: `.map()`, `.apply()`.\n",
    "    \n",
    "Here is an example of a simple benchmark, where the **vectorized version is 100x faster** than using a loop.  \n",
    "> ðŸŒˆ **Note:** `%timeit` is a Jupyter \"magic function\" that allows to benchmark a line of code.\n",
    "\n",
    "<br>\n",
    "\n",
    "```python\n",
    "# Define a function that updates a DataFrame using a loop -> an example of what NOT TO DO.\n",
    "def loop_method(df):\n",
    "    abbreviations = {\"S\": \"Southampton\", \"C\": \"Cherbourg\", \"Q\": \"Queenstown\"}\n",
    "    for index, value in enumerate(df[\"Embarked\"]):\n",
    "        df.loc[index, \"Embarked\"] = abbreviations.get(value, None)\n",
    "\n",
    "# Define a function that does the same `loop_method`, but using vectorization.\n",
    "def vectorized_method(df):\n",
    "    df.Embarked = df.Embarked.map({\"S\": \"Southampton\", \"C\": \"Cherbourg\", \"Q\": \"Queenstown\"})\n",
    "\n",
    "# Let's benchmark our two implementations.\n",
    "df = pd.read_csv(\"data/titanic.csv\")\n",
    "%timeit loop_method(df)               # -> 60.3 ms Â± 1.38 ms per loop\n",
    "df = pd.read_csv(\"data/titanic.csv\")\n",
    "%timeit vectorized_method(df)         # -> 534 Âµs Â± 23.2 Âµs per loop. More than 100x faster!\n",
    "```\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "### Micro-Exercise 4\n",
    "\n",
    "Here is a function that expands the abbreviated value for \"Port of embarkation\" (column name `Embarked`) in the Titanic dataset to the full name of the city (\"C\" for \"Cherbourg\", \"Q\" for \"Queenstown\", \"S\" for \"Southampton\").\n",
    "\n",
    "```python\n",
    "def expand_port_of_embarkation(input_value):\n",
    "    \"\"\"Converts the abbreviated port of embarkation to its full name.\"\"\"\n",
    "    abbreviations = {\"C\": \"Cherbourg\", \"Q\": \"Queenstown\", \"S\": \"Southampton\"}\n",
    "    return abbreviations.get(input_value, None) if len(str(input_value)) == 1 else input_value\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "* Using the above function, your task is to **add a new `Embarked_city` column** to the DataFrame that contains the full name\n",
    "  of the port of embarkation of each passenger.  \n",
    "* If needed, you can reload the Titanic dataset with: `df = pd.read_csv(\"data/titanic.csv\")`\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "[Back to ToC](#toc)\n",
    "\n",
    "# 3. Grouping data by factor <a id='a3' />\n",
    "---------------------\n",
    "\n",
    "When analyzing a dataset where some variables (columns) are factors (categorical values), it is often useful to group the samples (rows) by these factors.  \n",
    "This can be done using the method:\n",
    "\n",
    "* **`.groupby()`**: group data by one or more categorical columns.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "**Example:** grouping by a **single factor**.\n",
    "\n",
    "* We earlier computed the proportions of women and men that survived in the Titanic dataset.\n",
    "  Using **`.groupby()`** can make this a lot easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/titanic.csv\")\n",
    "df.groupby(\"Sex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operation creates a groupby object on which we can call summary functions (*eg,* `.sum()`, `.mean()`) or a custom function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Note:* since a mean value can only be computed for numeric values, the argument `numeric_only` must be\n",
    "> set to `True` so that any non-numeric column gets skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Sex\").mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do it for a specific column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Sex\").Age.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Example:** grouping by **multiple factors**.\n",
    "\n",
    "* Compute mean values by gender and passenger class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby([\"Sex\", \"Pclass\"]).mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "### Micro-Exercise 5\n",
    "\n",
    "* Compute survival rates by gender, and by passenger class.\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "[Back to ToC](#toc)\n",
    "\n",
    "\n",
    "# 4. Writing DataFrames to disk <a id='5'/>\n",
    "----------------------------------------------------------\n",
    "\n",
    "Just like when reading a file, **writing a DataFrame to disk** has several functions depending on the format in which the data should be stored.\n",
    "* **`to_csv()`**: write a DataFrame as comma-separated file, or any other separator-delimited format such as\n",
    "  tab-delimited. \n",
    "* **`to_excel()`**: write a DataFrame in Excel format.\n",
    "* **`to_html()`**: write a DataFrame in HTML format.\n",
    "* See [here for more writer functions...](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html).\n",
    "\n",
    "These functions take similar arguments as the reader functions. E.g. when using the `to_csv()` writer functions, some useful arguments are:\n",
    "\n",
    "* **`sep`**: the type of delimiter to use. By default, `sep=\",\"`.\n",
    "  To write a tab-delimited file e.g., one would set `sep=\"\\t\"`.\n",
    "* **`header=None`**: to not include the header in the exported file.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Example:** write the `df` DataFrame to a file named `my_data.csv` in the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"my_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "# Exercises\n",
    "-----------------\n",
    "\n",
    "Exercises are located in the dedicated notebook `exercises_course1.ipynb`.\n",
    "\n",
    "* **Exercise 1.1**\n",
    "* If you have time: **additional exercises 1.2 and 1.3**\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "[Back to ToC](#toc)\n",
    "\n",
    "# Supplementary material - additional topics, not covered in class <a id='a' />\n",
    "---------------------------\n",
    "\n",
    "Pandas is a very large library, and we can only cover part of it during the class.\n",
    "Here is a **collection of additional topics that are useful to know**.  \n",
    "This section will not be covered in the course, but you can read it on your own if you are interested.\n",
    "\n",
    "Topics presented in the supplementary material include (but are not limited to):\n",
    "\n",
    "* **Sorting** operations on dataframes.\n",
    "* **Dropping rows** with missing values.\n",
    "* **Grouping data** by factor.\n",
    "* **Creating** DataFrames and Series **from scratch**.\n",
    "* **Concatenate**, **merge** and **join** DataFrames.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Annex 1 - Sorting operations on dataframes  <a id='a1' />\n",
    "\n",
    "DataFrames can be sorted using:\n",
    "\n",
    "* **`sort_values()`**: to sort rows based one or more column(s).\n",
    "  * The name of the column to sort on must be passed to the function.\n",
    "  * To sort on multiple columns, a list of column names must be passed.\n",
    "* **`sort_index()`**: to sort rows based on the dataframe index.\n",
    "\n",
    "Both functions take the optional arguments:\n",
    "* `ascending=True`: sort in ascending (`True`, the default) or descending (`False`) order.\n",
    "* `inplace=False`: if `True` the original DataFrame is sorted, if `False` a sorted copy\n",
    "  of the DataFrame is returned (this is the default).\n",
    "\n",
    "<br>\n",
    "\n",
    "**Examples:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset with Unique Molecular Identifier (UMI) counts per gene.\n",
    "df = pd.read_table('data/pbmc_data.countMatrix.50.txt.zip', sep=\" \", index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* Sort the `df` DataFrame in descending order of the first column.  \n",
    "> ðŸŒˆ **Note:** to avoid having to pass the explicit column name, we use `df.columns[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(df.columns[0], ascending=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* Sort the `df` DataFrame in descending order of the first 3 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(list(df.columns[0:3]), ascending=False)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* Sort the DataFrame by index values using **`sort_index()`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.sort_index(ascending=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "[Back to ToC](#toc)\n",
    "\n",
    "## Annex 2 - Dropping rows with missing values (`NaN`) <a id='a2' />\n",
    "\n",
    "Datasets frequently contain rows with missing data, indicated as `NaN` or `NA` (stands for \"not a number\" - but it is used even if the column type is not numeric).\n",
    "\n",
    "In the titanic dataset for instance, `NaN` values are found in the columns `Age` and `Embarked`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/titanic.csv\")\n",
    "df.head(100).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove rows with `NaN` values, the **`dropna()`** method of a DataFrame can be used:\n",
    "* By default, `dropna()` drops rows containing `NaN` values. Columns can be dropped by passing `axis=1`\n",
    "  to the function.\n",
    "* By default, the method returns a **copy of the DataFrame**. Use `dropna(inplace=True)` to modify\n",
    "  the original DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna().head(80).tail()\n",
    "print(\"Number of rows in input DataFrame:\", df.shape[0])\n",
    "print(\"Number of rows after removing NaN values:\", df.dropna().shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "[Back to ToC](#toc)\n",
    "\n",
    "## Annex 4 - Creating DataFrames and Series from scratch <a id='a4' />\n",
    "\n",
    "### Creating DataFrames\n",
    "\n",
    "To create a new pandas DataFrame, we pass a `dict` (dictionary) to **`pd.DataFrame()`**, where:\n",
    "\n",
    "* The dictionary's **keys are column names**.\n",
    "* The dictionary's **values are the values for the given column**. The values can be either:\n",
    "    * A sequence - e.g. a `list` or a `tuple`.\n",
    "    * A unique value - in which case all values in the column will be identical.\n",
    "  \n",
    "By default, a numeric index is auto-generated with values starting at `0` (corresponding to row positions).  \n",
    "A custom index can be set by adding the `index` argument when instantiating the DataFrame, or the default index can be replaced by a custom index after the DataFrame is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"Color\": (\"red\", \"green\", \"pink\", \"green\"),\n",
    "        \"Speed\": [8, 7, 5, 5],\n",
    "        \"Jump\": [7, 8, 7, 5]\n",
    "    },\n",
    "    index=[\"Mario\", \"Luigi\", \"Peach\", \"Yoshi\"]\n",
    ")\n",
    "\n",
    "# Note: evaluating a DataFrame object at the end of a jupyter cell will render it nicely in the notebook.\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a pandas Series <a id='1.2'></a>\n",
    "\n",
    "To create a new pandas **Series**, we pass a sequence (e.g. list, tuple, dict, generator) to **`pd.Series()`**:\n",
    "* The optional `name` argument allows to associate a \"name\" to the Series.\n",
    "* As with `pd.DataFrame()`, an optional `index` argument can be passed (by default the index\n",
    "  is set to numerical values starting from `0`).\n",
    "* Alternatively, a `dict` can be passed as input to `pd.Series()`, in which case the keys of the dictionary\n",
    "  are used as index values, and the values as values for the Series.\n",
    "  \n",
    "The basic characteristics of a Series (here named `s`) are:\n",
    "* Its **length**: retrieved with **`s.size`** or `len(s)`\n",
    "* Its **name**: retrieved with **`s.name`**.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_player = pd.Series(\n",
    "    [\"Toad\", \"white\", \"10\"],\n",
    "    index=[\"Color\", \"Speed\", \"Jump\"],\n",
    "    name=\"Toad\",\n",
    ")\n",
    "\n",
    "new_player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Adding a Series as a row to a DataFrame with `pd.concat()`\n",
    "\n",
    "**`pd.concat()`** is a method generally used to concatenate DataFrames (either along rows or columns). But it can also be used to add a Series as a new row to an existing DataFrame.\n",
    "\n",
    "Note that:\n",
    "* We are actually converting the Series to DataFrame with `.to_frame()`, and transpose it with `.T`.\n",
    "* The function does **not modify the existing DataFrame/Series** given in input, but **returns a new one**.\n",
    "* For the concatenation to work, the **index of the Series must correspond to the column names of the \n",
    "  DataFrame**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat((df, new_player.to_frame().T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "[Back to ToC](#toc)\n",
    "\n",
    "## Annex 5 - Concatenate, merge and join DataFrames  <a id='a5' />\n",
    "\n",
    "### DataFrame concatenation\n",
    "\n",
    "The **`.concat()`** method allows to **concatenate two or more DataFrames**.  \n",
    "The `axis` argument allows to specify whether the concatenation should be along rows or columns.\n",
    "\n",
    "* **`.concat([df1, df2, ...], axis=0)`**: concatenate rows of the DataFrames. This is the default.\n",
    "* **`.concat([df1, df2, ...], axis=1)`**: concatenate columns of the DataFrames.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "* Let's start by creating a few small DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(\n",
    "    {\n",
    "        \"Color\": [\"red\", \"green\", \"pink\", \"green\"],\n",
    "        \"Speed\": [8, 7, 5, 5],\n",
    "        \"Jump\": [7, 8, 7, 5]\n",
    "    },\n",
    "    index=[\"Mario\", \"Luigi\", \"Peach\", \"Yoshi\"]\n",
    ")\n",
    "df2 = pd.DataFrame(\n",
    "    {\n",
    "        \"Color\": (\"white\", \"yellow\"),\n",
    "        \"Speed\": [10, 3],\n",
    "        \"Jump\": [7, 3]\n",
    "    },\n",
    "    index=[\"Toad\", \"Bowser\"]\n",
    ")\n",
    "df3 = pd.DataFrame(\n",
    "    {\n",
    "        \"Strength\": [5, 10, 6],\n",
    "        \"Groove\": [7, 1, 10],\n",
    "    },\n",
    "    index=[\"Mario\", \"Bowser\", \"Yoshi\"]\n",
    ")\n",
    "\n",
    "print(df1)\n",
    "print(df2)\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* We can now concatenate `df1` and `df2` by rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `\"outer\"` vs. `\"inner\"` joining\n",
    "\n",
    "* By default, `.pd.concat()` uses the value **`join=\"outer\"`** to concatenate DataFrames.\n",
    "  This means that all rows/columns of both DataFrames are kept, and `NaN` values are inserted\n",
    "  in missing fields. In other words, it takes the **union** of both DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1, df3], axis=1, join=\"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using **`join=\"inner\"`** only keeps rows with an index that is present in both DataFrames.\n",
    "  In other words, it takes the **intersection** of both DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.concat([df1, df3], axis=1, join=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Merge and join\n",
    "\n",
    "The **[`merge()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html)** and **[`join()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.join.html)** methods allow to combine DataFrames, linking their rows based on a common column (also referred to as a **key**).\n",
    "\n",
    "To illustrate these 2 methods, let's create 2 DataFrames that we can merge.\n",
    "> ðŸŒˆ **Note:** this also illustrates how **a dataframe can be constructed from a dictionary** data structure.\n",
    "> * The dictionary keys are treated as column names, and the list of values associated with a key is\n",
    ">   treated as list of elements in the corresponding column. Note that all columns should have the same\n",
    ">   number of elements (or a single element, in which case all rows of the column are assigned this same\n",
    ">   element).  \n",
    "> * If no index is specified, pandas uses its default indexing, i.e. row positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(\n",
    "    {\n",
    "        \"key\": [\"b\", \"b\", \"a\", \"c\", \"a\", \"a\", \"b\"], \n",
    "        \"data1\": range(7)\n",
    "    }\n",
    ")\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({\n",
    "    \"key\": [\"a\", \"b\", \"d\"], \n",
    "    \"data2\": range(3)\n",
    "})\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Let's now merge the two data frames**, with the default application of the **`.merge()`** method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How has python interpreted our call to `.merge()`?\n",
    "\n",
    "1. It has assumed that we want to merge on the basis of the common `key` column.\n",
    "2. It has identified the values of `key` which occur in both dataframes.\n",
    "3. It has generated a dataframe with all combinations of rows from dataframes 1 and 2 that are \n",
    "   associated with a particular `key` value.\n",
    "\n",
    "We can be more precise by specifying how to merge the dataframes, using the **`on`** option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df2, on=\"key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**By default, `.merge` performs an \"inner\" operation**, taking the intersection of the key sets. However, we can specify the way we want to merge by passing `\"outer\"`, `\"left\"`, `\"right\"` to the **`how`** argument. This determines which set of keys to consider (the union of the two sets, all of those that occur in df1, all of those that occur in df2). Missing values show up as `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df2, on=\"key\", how=\"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Merging can also be done based on the index values**. Let's illustrate this using another dataset:\n",
    "\n",
    "* First let's create 2 DataFrames to merge: `df_A` and `df_B`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table('data/pbmc_data.countMatrix.50.txt.zip', sep=\" \", index_col=0)\n",
    "\n",
    "# Split the columns based on whether or not they start with \"AAAT\".\n",
    "mask = df.columns.str.startswith(\"AAAT\")\n",
    "AAAT_cols = df.columns[ mask ]\n",
    "nonAAAT_cols = df.columns[ ~mask ]\n",
    "\n",
    "df_A = df[AAAT_cols]\n",
    "df_B = df[nonAAAT_cols]\n",
    "\n",
    "df_A.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_B.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* Merge the two DataFrames based on the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(df_A, df_B, left_index=True, right_index=True)\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "[Back to ToC](#toc)\n",
    "\n",
    "## Annex 6 - Mixed selection by names and positions for `.loc[]` and `.iloc[]` <a id='a6' />\n",
    "    \n",
    "A frequent situation is that we want to select rows based on a certain condition, e.g. `df[\"Age\"] <= 35`, \n",
    "and columns based on position , e.g. `1:3` to select the second and third columns. The problem is then the following:\n",
    "* **`.loc[]`** does not support column selection by position: it requires index values, i.e. column\n",
    "  and row names.\n",
    "* **`.iloc[]`** does not support boolean results (`True`/`False`) for row selection: it requires to get\n",
    "  a position.\n",
    "  \n",
    "Possible solutions to mixed indexing are:\n",
    "\n",
    "* Use the **`.columns` attribute** to get the names of columns, which can then be used with `.loc[]`.\n",
    "  ```python\n",
    "  df.loc[ df[\"Age\"] <= 35, df.columns[1:3] ]\n",
    "  ```\n",
    "* If the **index values correspond to row positions (0, 1, 2, ...)**, the `.index` attribute can be \n",
    "  used to get row positions and use them with `.iloc[]`:\n",
    "  ```python\n",
    "  df.iloc[ df[df[\"Age\"] <= 35].index, 1:3 ]\n",
    "  ```\n",
    "* With `.iloc[]`, use the **`.query()` method** to select rows:\n",
    "  ```python\n",
    "  df.iloc[:, 0:6].query(\"Age <= 35\")\n",
    "  ```\n",
    "\n",
    "<br>\n",
    "\n",
    "**Examples:**\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select passengers younger than 35 and the 5 first columns:\n",
    "df = pd.read_csv(\"data/titanic.csv\")\n",
    "df.loc[df[\"Age\"] < 35, df.columns[0:5]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above, but using the \"iloc[]\" indexer:\n",
    "df.iloc[:, 0:6].query(\"Age < 35\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "[Back to ToC](#toc)\n",
    "\n",
    "## Annex 7: more ways to add rows to a DataFrame <a id='a7' />\n",
    "\n",
    "Several options exist to add or edit rows of a DataFrame:\n",
    "\n",
    "* As we have already seen, the easiest way is to **assign a sequence of values to a new row**\n",
    "  (or an existing one to overwrite it).  \n",
    "  **Important:** the sequence must have the same length as there are columns in the DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/titanic.csv\")\n",
    "\n",
    "# Add a new row to the DataFrame.\n",
    "df.loc[len(df),:] = [\"Bob\", \"male\", 27, 3, 1, 0, 10, \"S\"]\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "A row can also be added by **assigning a `Series` to a new row** (or an exiting row, to overwrite it).  \n",
    "When using this method:\n",
    "* The names used in the Series' index must match with the column names.\n",
    "* Missing fields are allowed.\n",
    "* The order of fields has no importance, since they are named.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_passenger = pd.Series({\"Name\": \"Alice\", \"Sex\": \"female\", \"Embarked\": \"C\", \"Fare\": 95})\n",
    "df.loc[len(df),:] = new_passenger\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Another option is to **concatenate a DataFrame** with another `DataFrame` or `Series` using  **`pd.concat()`**, which takes a list of `DataFrame` or `Series`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "            {\n",
    "                \"Name\": \"Chuck Norris\",\n",
    "                \"Sex\": \"fluid\",\n",
    "                \"Age\": pd.NA,\n",
    "                \"Pclass\": 2,\n",
    "                \"Survived\": 2,\n",
    "                \"Family\": pd.NA,\n",
    "                \"Fare\": 0,\n",
    "                \"Embarked\": \"S\"\n",
    "            },\n",
    "        index=[df.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(\n",
    "    [\n",
    "        df,\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"Name\": \"Chuck Norris\",\n",
    "                \"Sex\": \"fluid\",\n",
    "                \"Pclass\": 2,\n",
    "                \"Survived\": 2,\n",
    "                \"Fare\": 0,\n",
    "                \"Embarked\": \"S\"\n",
    "            },\n",
    "        index=[df.shape[0]])\n",
    "    ]\n",
    ")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Adding a row at specific location** is possible, if a little hacky as you first have to add the row at the\n",
    "end of the DataFrame, and then re-order the rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[890.5,:] = [\"Bob Jr.\", \"male\", 2, 3, 1, 1, 10, \"S\"]\n",
    "df.sort_index(inplace=True)\n",
    "df.reset_index(inplace=True, drop=True)  # With drop=True, the index is reset to the default index values.\n",
    "df.tail()\n",
    "\n",
    "# Alternatively: df = df.sort_index().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "[Back to ToC](#toc)\n",
    "\n",
    "## Annex 8 - Wide and long format <a id='a8' />\n",
    "\n",
    "Most dataset are usually stored in so-called **wide format**, because it is more efficient and avoids duplication of information in the table. However, it can sometimes be interesting to go from wide to long or long to wide, because some operations are easier on one format or the other.\n",
    "\n",
    "**Wide format:**\n",
    "\n",
    "| Id     | mol1   | mol2    |\n",
    "| ------ |:------:| -------:|\n",
    "| a      | 1.0    | 10.0    |\n",
    "| b      | 2.0    |   20.0  |\n",
    "| c      | 3.0    |    30.0 |\n",
    "\n",
    "**Long format:**\n",
    "\n",
    "| Id     | Value  | Attr    |\n",
    "| ------ |:------:| -------:|\n",
    "| a      | 1.0    | mol1    |\n",
    "| a      | 10.0   | mol2    |\n",
    "| b      | 2.0    | mol1    |\n",
    "| b      | 20.0   | mol2    |\n",
    "| c      | 3.0    | mol1    |\n",
    "| c      | 30.0   | mol2    |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(\"data/pbmc_data.countMatrix.50.txt.zip\", index_col=0,sep=' ')\n",
    "\n",
    "## data in wide format.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The conversion to the **long** format is done using the **`.melt()`** method of DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"gene\"] = df.index\n",
    "df_long = pd.melt(df, id_vars=[\"gene\"])  # Convert to long format, with gene as identifiers.\n",
    "df_long.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Having the data in long format now allows us to do this, which was a bit difficult otherwise:  \n",
    "> ðŸŒˆ **Note:** running the cell below can take a few seconds (notice the \"[ \\* ]\" to the left of the Jupyter Notebook cell, indicating that the cell is computing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "df_long[\"logVal\"] = np.log10(10**0 + df_long[\"value\"])\n",
    "\n",
    "g = sns.catplot(\n",
    "    x=\"logVal\", y=\"variable\",\n",
    "    orient=\"horizontal\" , data=df_long, \n",
    "    kind=\"bar\", aspect=2, height=7\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
